*** File /cygdrive/c/Users/Paul/Documents/My Dropbox/Online Resources/My Homework/UBC 2011W/NSERC/matrix-factor/./unsupported/Eigen/src/SparseExtra/MarketIO.h:
                // This file is part of Eigen, a lightweight C++ template library
                // for linear algebra.
                //
                // Copyright (C) 2011 Gael Guennebaud <gael.guennebaud@inria.fr>
                //
                // Eigen is free software; you can redistribute it and/or
                // modify it under the terms of the GNU Lesser General Public
                // License as published by the Free Software Foundation; either
                // version 3 of the License, or (at your option) any later version.
                //
                // Alternatively, you can redistribute it and/or
                // modify it under the terms of the GNU General Public License as
                // published by the Free Software Foundation; either version 2 of
                // the License, or (at your option) any later version.
                //
                // Eigen is distributed in the hope that it will be useful, but WITHOUT ANY
                // WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
                // FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License or the
                // GNU General Public License for more details.
                //
                // You should have received a copy of the GNU Lesser General Public
                // License and a copy of the GNU General Public License along with
                // Eigen. If not, see <http://www.gnu.org/licenses/>.
                
                #ifndef EIGEN_SPARSE_MARKET_IO_H
                #define EIGEN_SPARSE_MARKET_IO_H
                
                
                template<typename SparseMatrixType>
                bool loadMarket(SparseMatrixType& mat, const std::string& filename)
           1 -> {
                  typedef typename SparseMatrixType::Scalar Scalar;
                  std::ifstream input(filename.c_str(),std::ios::in);
                  if(!input)
                    return false;
                  
                  const int maxBuffersize = 2048;
                  char buffer[maxBuffersize];
                  
                  bool readsizes = false;
                  
                  int M(-1), N(-1), NNZ(-1);
                  int count = 0;
                  
                  while(input.getline(buffer, maxBuffersize))
                  {
                    // skip comments
                    if(buffer[0]=='%')
                      continue;
                    
                    std::stringstream line(buffer);
                    
                    if(!readsizes)
                    {
                      line >> M >> N >> NNZ;
                      readsizes = true;
                      std::cout << "sizes: " << M << "," << N << "," << NNZ << "\n";
                      mat.resize(M,N);
                      mat.reserve(NNZ);
                    }
                    else
                    {
                      int i(-1), j(-1);
                      Scalar v;
                      line >> i >> j >> v;
                      i--;
                      j--;
                      if(i>=0 && j>=0 && i<M && j<N)
                      {
                        ++ count;
                        //std::cout << "M[" << i << "," << j << "] = " << v << "\n";
                        mat.insert(i,j) = v;
                      }
                      else
                        std::cerr << "Invalid read: " << i << "," << j << "\n";
                    }
                  }
                  
                  if(count!=NNZ)
                    std::cerr << count << "!=" << NNZ << "\n";
                  
                  input.close();
                  return true;
                }
                
                template<typename SparseMatrixType>
                bool saveMarket(const SparseMatrixType& mat, const std::string& filename)
                {
                  std::ofstream out(filename.c_str(),std::ios::out);
                  if(!out)
                    return false;
                  
                  out.flags(std::ios_base::scientific);
                  out.precision(64);
                  out << mat.rows() << " " << mat.cols() << " " << mat.nonZeros() << "\n";
                  int count = 0;
                  for(int j=0; j<mat.outerSize(); ++j)
                    for(typename SparseMatrixType::InnerIterator it(mat,j); it; ++it)
                    {
                      ++ count;
                      out << it.row()+1 << " " << it.col()+1 << " " << it.value() << "\n";
                    }
                  out.close();
                  return true;
                }
                
                #endif // EIGEN_SPARSE_MARKET_IO_H


Top 10 Lines:

     Line      Count

       31          1

Execution Summary:

        1   Executable lines in this file
        1   Lines executed
   100.00   Percent of the file executed

        1   Total number of line executions
     1.00   Average executions per line


*** File /cygdrive/c/Users/Paul/Documents/My Dropbox/Online Resources/My Homework/UBC 2011W/NSERC/matrix-factor/./Eigen/src/SparseCore/SparseDiagonalProduct.h:
                // This file is part of Eigen, a lightweight C++ template library
                // for linear algebra.
                //
                // Copyright (C) 2009 Gael Guennebaud <gael.guennebaud@inria.fr>
                //
                // Eigen is free software; you can redistribute it and/or
                // modify it under the terms of the GNU Lesser General Public
                // License as published by the Free Software Foundation; either
                // version 3 of the License, or (at your option) any later version.
                //
                // Alternatively, you can redistribute it and/or
                // modify it under the terms of the GNU General Public License as
                // published by the Free Software Foundation; either version 2 of
                // the License, or (at your option) any later version.
                //
                // Eigen is distributed in the hope that it will be useful, but WITHOUT ANY
                // WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
                // FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License or the
                // GNU General Public License for more details.
                //
                // You should have received a copy of the GNU Lesser General Public
                // License and a copy of the GNU General Public License along with
                // Eigen. If not, see <http://www.gnu.org/licenses/>.
                
                #ifndef EIGEN_SPARSE_DIAGONAL_PRODUCT_H
                #define EIGEN_SPARSE_DIAGONAL_PRODUCT_H
                
                // The product of a diagonal matrix with a sparse matrix can be easily
                // implemented using expression template.
                // We have two consider very different cases:
                // 1 - diag * row-major sparse
                //     => each inner vector <=> scalar * sparse vector product
                //     => so we can reuse CwiseUnaryOp::InnerIterator
                // 2 - diag * col-major sparse
                //     => each inner vector <=> densevector * sparse vector cwise product
                //     => again, we can reuse specialization of CwiseBinaryOp::InnerIterator
                //        for that particular case
                // The two other cases are symmetric.
                
                namespace internal {
                
                template<typename Lhs, typename Rhs>
                struct traits<SparseDiagonalProduct<Lhs, Rhs> >
                {
                  typedef typename remove_all<Lhs>::type _Lhs;
                  typedef typename remove_all<Rhs>::type _Rhs;
                  typedef typename _Lhs::Scalar Scalar;
                  typedef typename promote_index_type<typename traits<Lhs>::Index,
                                                         typename traits<Rhs>::Index>::type Index;
                  typedef Sparse StorageKind;
                  typedef MatrixXpr XprKind;
                  enum {
                    RowsAtCompileTime = _Lhs::RowsAtCompileTime,
                    ColsAtCompileTime = _Rhs::ColsAtCompileTime,
                
                    MaxRowsAtCompileTime = _Lhs::MaxRowsAtCompileTime,
                    MaxColsAtCompileTime = _Rhs::MaxColsAtCompileTime,
                
                    SparseFlags = is_diagonal<_Lhs>::ret ? int(_Rhs::Flags) : int(_Lhs::Flags),
                    Flags = (SparseFlags&RowMajorBit),
                    CoeffReadCost = Dynamic
                  };
                };
                
                enum {SDP_IsDiagonal, SDP_IsSparseRowMajor, SDP_IsSparseColMajor};
                template<typename Lhs, typename Rhs, typename SparseDiagonalProductType, int RhsMode, int LhsMode>
                class sparse_diagonal_product_inner_iterator_selector;
                
                } // end namespace internal
                
                template<typename Lhs, typename Rhs>
                class SparseDiagonalProduct
                  : public SparseMatrixBase<SparseDiagonalProduct<Lhs,Rhs> >,
                    internal::no_assignment_operator
                {
                    typedef typename Lhs::Nested LhsNested;
                    typedef typename Rhs::Nested RhsNested;
                
                    typedef typename internal::remove_all<LhsNested>::type _LhsNested;
                    typedef typename internal::remove_all<RhsNested>::type _RhsNested;
                
                    enum {
                      LhsMode = internal::is_diagonal<_LhsNested>::ret ? internal::SDP_IsDiagonal
                              : (_LhsNested::Flags&RowMajorBit) ? internal::SDP_IsSparseRowMajor : internal::SDP_IsSparseColMajor,
                      RhsMode = internal::is_diagonal<_RhsNested>::ret ? internal::SDP_IsDiagonal
                              : (_RhsNested::Flags&RowMajorBit) ? internal::SDP_IsSparseRowMajor : internal::SDP_IsSparseColMajor
                    };
                
                  public:
                
                    EIGEN_SPARSE_PUBLIC_INTERFACE(SparseDiagonalProduct)
                
                    typedef internal::sparse_diagonal_product_inner_iterator_selector
                                <_LhsNested,_RhsNested,SparseDiagonalProduct,LhsMode,RhsMode> InnerIterator;
                
                    EIGEN_STRONG_INLINE SparseDiagonalProduct(const Lhs& lhs, const Rhs& rhs)
                      : m_lhs(lhs), m_rhs(rhs)
                    {
                      eigen_assert(lhs.cols() == rhs.rows() && "invalid sparse matrix * diagonal matrix product");
                    }
                
                    EIGEN_STRONG_INLINE Index rows() const { return m_lhs.rows(); }
                    EIGEN_STRONG_INLINE Index cols() const { return m_rhs.cols(); }
                
                    EIGEN_STRONG_INLINE const _LhsNested& lhs() const { return m_lhs; }
                    EIGEN_STRONG_INLINE const _RhsNested& rhs() const { return m_rhs; }
                
                  protected:
                    LhsNested m_lhs;
                    RhsNested m_rhs;
                };
                
                namespace internal {
                
                template<typename Lhs, typename Rhs, typename SparseDiagonalProductType>
                class sparse_diagonal_product_inner_iterator_selector
                <Lhs,Rhs,SparseDiagonalProductType,SDP_IsDiagonal,SDP_IsSparseRowMajor>
                  : public CwiseUnaryOp<scalar_multiple_op<typename Lhs::Scalar>,const Rhs>::InnerIterator
                {
                    typedef typename CwiseUnaryOp<scalar_multiple_op<typename Lhs::Scalar>,const Rhs>::InnerIterator Base;
                    typedef typename Lhs::Index Index;
                  public:
                    inline sparse_diagonal_product_inner_iterator_selector(
                              const SparseDiagonalProductType& expr, Index outer)
                      : Base(expr.rhs()*(expr.lhs().diagonal().coeff(outer)), outer)
                    {}
                };
                
                template<typename Lhs, typename Rhs, typename SparseDiagonalProductType>
                class sparse_diagonal_product_inner_iterator_selector
                <Lhs,Rhs,SparseDiagonalProductType,SDP_IsDiagonal,SDP_IsSparseColMajor>
                  : public CwiseBinaryOp<
                      scalar_product_op<typename Lhs::Scalar>,
                      SparseInnerVectorSet<Rhs,1>,
                      typename Lhs::DiagonalVectorType>::InnerIterator
                {
                    typedef typename CwiseBinaryOp<
                      scalar_product_op<typename Lhs::Scalar>,
                      SparseInnerVectorSet<Rhs,1>,
                      typename Lhs::DiagonalVectorType>::InnerIterator Base;
                    typedef typename Lhs::Index Index;
                  public:
                    inline sparse_diagonal_product_inner_iterator_selector(
                              const SparseDiagonalProductType& expr, Index outer)
                      : Base(expr.rhs().innerVector(outer) .cwiseProduct(expr.lhs().diagonal()), 0)
                    {}
                };
                
                template<typename Lhs, typename Rhs, typename SparseDiagonalProductType>
                class sparse_diagonal_product_inner_iterator_selector
                <Lhs,Rhs,SparseDiagonalProductType,SDP_IsSparseColMajor,SDP_IsDiagonal>
                  : public CwiseUnaryOp<scalar_multiple_op<typename Rhs::Scalar>,const Lhs>::InnerIterator
                {
                    typedef typename CwiseUnaryOp<scalar_multiple_op<typename Rhs::Scalar>,const Lhs>::InnerIterator Base;
                    typedef typename Lhs::Index Index;
                  public:
                    inline sparse_diagonal_product_inner_iterator_selector(
                              const SparseDiagonalProductType& expr, Index outer)
                      : Base(expr.lhs()*expr.rhs().diagonal().coeff(outer), outer)
                    {}
                };
                
                template<typename Lhs, typename Rhs, typename SparseDiagonalProductType>
                class sparse_diagonal_product_inner_iterator_selector
                <Lhs,Rhs,SparseDiagonalProductType,SDP_IsSparseRowMajor,SDP_IsDiagonal>
                  : public CwiseBinaryOp<
                      scalar_product_op<typename Rhs::Scalar>,
                      SparseInnerVectorSet<Lhs,1>,
                      Transpose<const typename Rhs::DiagonalVectorType> >::InnerIterator
                {
                    typedef typename CwiseBinaryOp<
                      scalar_product_op<typename Rhs::Scalar>,
                      SparseInnerVectorSet<Lhs,1>,
                      Transpose<const typename Rhs::DiagonalVectorType> >::InnerIterator Base;
                    typedef typename Lhs::Index Index;
                  public:
                    inline sparse_diagonal_product_inner_iterator_selector(
                              const SparseDiagonalProductType& expr, Index outer)
                      : Base(expr.lhs().innerVector(outer) .cwiseProduct(expr.rhs().diagonal().transpose()), 0)
                    {}
                };
                
                } // end namespace internal
                
                // SparseMatrixBase functions
                
                template<typename Derived>
                template<typename OtherDerived>
                const SparseDiagonalProduct<Derived,OtherDerived>
                SparseMatrixBase<Derived>::operator*(const DiagonalBase<OtherDerived> &other) const
           1 -> {
                  return SparseDiagonalProduct<Derived,OtherDerived>(this->derived(), other.derived());
                }
                
                #endif // EIGEN_SPARSE_DIAGONAL_PRODUCT_H


Top 10 Lines:

     Line      Count

      191          1

Execution Summary:

        1   Executable lines in this file
        1   Lines executed
   100.00   Percent of the file executed

        1   Total number of line executions
     1.00   Average executions per line


*** File /cygdrive/c/Users/Paul/Documents/My Dropbox/Online Resources/My Homework/UBC 2011W/NSERC/matrix-factor/./Eigen/src/SparseCore/SparseBlock.h:
                // This file is part of Eigen, a lightweight C++ template library
                // for linear algebra.
                //
                // Copyright (C) 2008-2009 Gael Guennebaud <gael.guennebaud@inria.fr>
                //
                // Eigen is free software; you can redistribute it and/or
                // modify it under the terms of the GNU Lesser General Public
                // License as published by the Free Software Foundation; either
                // version 3 of the License, or (at your option) any later version.
                //
                // Alternatively, you can redistribute it and/or
                // modify it under the terms of the GNU General Public License as
                // published by the Free Software Foundation; either version 2 of
                // the License, or (at your option) any later version.
                //
                // Eigen is distributed in the hope that it will be useful, but WITHOUT ANY
                // WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
                // FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License or the
                // GNU General Public License for more details.
                //
                // You should have received a copy of the GNU Lesser General Public
                // License and a copy of the GNU General Public License along with
                // Eigen. If not, see <http://www.gnu.org/licenses/>.
                
                #ifndef EIGEN_SPARSE_BLOCK_H
                #define EIGEN_SPARSE_BLOCK_H
                
                namespace internal {
                template<typename MatrixType, int Size>
                struct traits<SparseInnerVectorSet<MatrixType, Size> >
                {
                  typedef typename traits<MatrixType>::Scalar Scalar;
                  typedef typename traits<MatrixType>::Index Index;
                  typedef typename traits<MatrixType>::StorageKind StorageKind;
                  typedef MatrixXpr XprKind;
                  enum {
                    IsRowMajor = (int(MatrixType::Flags)&RowMajorBit)==RowMajorBit,
                    Flags = MatrixType::Flags,
                    RowsAtCompileTime = IsRowMajor ? Size : MatrixType::RowsAtCompileTime,
                    ColsAtCompileTime = IsRowMajor ? MatrixType::ColsAtCompileTime : Size,
                    MaxRowsAtCompileTime = RowsAtCompileTime,
                    MaxColsAtCompileTime = ColsAtCompileTime,
                    CoeffReadCost = MatrixType::CoeffReadCost
                  };
                };
                } // end namespace internal
                
                template<typename MatrixType, int Size>
                class SparseInnerVectorSet : internal::no_assignment_operator,
                  public SparseMatrixBase<SparseInnerVectorSet<MatrixType, Size> >
                {
                  public:
                
                    enum { IsRowMajor = internal::traits<SparseInnerVectorSet>::IsRowMajor };
                
                    EIGEN_SPARSE_PUBLIC_INTERFACE(SparseInnerVectorSet)
                    class InnerIterator: public MatrixType::InnerIterator
                    {
                      public:
                        inline InnerIterator(const SparseInnerVectorSet& xpr, Index outer)
                          : MatrixType::InnerIterator(xpr.m_matrix, xpr.m_outerStart + outer), m_outer(outer)
                        {}
                        inline Index row() const { return IsRowMajor ? m_outer : this->index(); }
                        inline Index col() const { return IsRowMajor ? this->index() : m_outer; }
                      protected:
                        Index m_outer;
                    };
                
                    inline SparseInnerVectorSet(const MatrixType& matrix, Index outerStart, Index outerSize)
                      : m_matrix(matrix), m_outerStart(outerStart), m_outerSize(outerSize)
                    {
                      eigen_assert( (outerStart>=0) && ((outerStart+outerSize)<=matrix.outerSize()) );
                    }
                
                    inline SparseInnerVectorSet(const MatrixType& matrix, Index outer)
                      : m_matrix(matrix), m_outerStart(outer), m_outerSize(Size)
                    {
                      eigen_assert(Size!=Dynamic);
                      eigen_assert( (outer>=0) && (outer<matrix.outerSize()) );
                    }
                
                //     template<typename OtherDerived>
                //     inline SparseInnerVectorSet& operator=(const SparseMatrixBase<OtherDerived>& other)
                //     {
                //       return *this;
                //     }
                
                //     template<typename Sparse>
                //     inline SparseInnerVectorSet& operator=(const SparseMatrixBase<OtherDerived>& other)
                //     {
                //       return *this;
                //     }
                
                    EIGEN_STRONG_INLINE Index rows() const { return IsRowMajor ? m_outerSize.value() : m_matrix.rows(); }
                    EIGEN_STRONG_INLINE Index cols() const { return IsRowMajor ? m_matrix.cols() : m_outerSize.value(); }
                
                  protected:
                
                    const typename MatrixType::Nested m_matrix;
                    Index m_outerStart;
                    const internal::variable_if_dynamic<Index, Size> m_outerSize;
                };
                
                
                /***************************************************************************
                * specialisation for SparseMatrix
                ***************************************************************************/
                
                template<typename _Scalar, int _Options, typename _Index, int Size>
                class SparseInnerVectorSet<SparseMatrix<_Scalar, _Options, _Index>, Size>
                  : public SparseMatrixBase<SparseInnerVectorSet<SparseMatrix<_Scalar, _Options, _Index>, Size> >
                {
                    typedef SparseMatrix<_Scalar, _Options, _Index> MatrixType;
                  public:
                
                    enum { IsRowMajor = internal::traits<SparseInnerVectorSet>::IsRowMajor };
                
                    EIGEN_SPARSE_PUBLIC_INTERFACE(SparseInnerVectorSet)
                    class InnerIterator: public MatrixType::InnerIterator
                    {
                      public:
                        inline InnerIterator(const SparseInnerVectorSet& xpr, Index outer)
                          : MatrixType::InnerIterator(xpr.m_matrix, xpr.m_outerStart + outer), m_outer(outer)
                        {}
                        inline Index row() const { return IsRowMajor ? m_outer : this->index(); }
                        inline Index col() const { return IsRowMajor ? this->index() : m_outer; }
                      protected:
                        Index m_outer;
                    };
                
                    inline SparseInnerVectorSet(const MatrixType& matrix, Index outerStart, Index outerSize)
                      : m_matrix(matrix), m_outerStart(outerStart), m_outerSize(outerSize)
                    {
                      eigen_assert( (outerStart>=0) && ((outerStart+outerSize)<=matrix.outerSize()) );
                    }
                
                    inline SparseInnerVectorSet(const MatrixType& matrix, Index outer)
                      : m_matrix(matrix), m_outerStart(outer), m_outerSize(Size)
                    {
                      eigen_assert(Size==1);
                      eigen_assert( (outer>=0) && (outer<matrix.outerSize()) );
                    }
                
                    template<typename OtherDerived>
                    inline SparseInnerVectorSet& operator=(const SparseMatrixBase<OtherDerived>& other)
                    {
                      typedef typename internal::remove_all<typename MatrixType::Nested>::type _NestedMatrixType;
                      _NestedMatrixType& matrix = const_cast<_NestedMatrixType&>(m_matrix);;
                      // This assignement is slow if this vector set is not empty
                      // and/or it is not at the end of the nonzeros of the underlying matrix.
                
                      // 1 - eval to a temporary to avoid transposition and/or aliasing issues
                      SparseMatrix<Scalar, IsRowMajor ? RowMajor : ColMajor, Index> tmp(other);
                
                      // 2 - let's check whether there is enough allocated memory
                      Index nnz           = tmp.nonZeros();
                      Index nnz_previous  = nonZeros();
                      Index free_size     = Index(matrix.data().allocatedSize()) + nnz_previous;
                      Index nnz_head      = m_outerStart==0 ? 0 : matrix.outerIndexPtr()[m_outerStart];
                      Index tail          = m_matrix.outerIndexPtr()[m_outerStart+m_outerSize.value()];
                      Index nnz_tail      = matrix.nonZeros() - tail;
                
                      if(nnz>free_size)
                      {
                        // realloc manually to reduce copies
                        typename MatrixType::Storage newdata(m_matrix.nonZeros() - nnz_previous + nnz);
                
                        std::memcpy(&newdata.value(0), &m_matrix.data().value(0), nnz_head*sizeof(Scalar));
                        std::memcpy(&newdata.index(0), &m_matrix.data().index(0), nnz_head*sizeof(Index));
                
                        std::memcpy(&newdata.value(nnz_head), &tmp.data().value(0), nnz*sizeof(Scalar));
                        std::memcpy(&newdata.index(nnz_head), &tmp.data().index(0), nnz*sizeof(Index));
                
                        std::memcpy(&newdata.value(nnz_head+nnz), &matrix.data().value(tail), nnz_tail*sizeof(Scalar));
                        std::memcpy(&newdata.index(nnz_head+nnz), &matrix.data().index(tail), nnz_tail*sizeof(Index));
                
                        matrix.data().swap(newdata);
                      }
                      else
                      {
                        // no need to realloc, simply copy the tail at its respective position and insert tmp
                        matrix.data().resize(nnz_head + nnz + nnz_tail);
                
                        if(nnz<nnz_previous)
                        {
                          std::memcpy(&matrix.data().value(nnz_head+nnz), &matrix.data().value(tail), nnz_tail*sizeof(Scalar));
                          std::memcpy(&matrix.data().index(nnz_head+nnz), &matrix.data().index(tail), nnz_tail*sizeof(Index));
                        }
                        else
                        {
                          for(Index i=nnz_tail-1; i>=0; --i)
                          {
                            matrix.data().value(nnz_head+nnz+i) = matrix.data().value(tail+i);
                            matrix.data().index(nnz_head+nnz+i) = matrix.data().index(tail+i);
                          }
                        }
                
                        std::memcpy(&matrix.data().value(nnz_head), &tmp.data().value(0), nnz*sizeof(Scalar));
                        std::memcpy(&matrix.data().index(nnz_head), &tmp.data().index(0), nnz*sizeof(Index));
                      }
                
                      // update outer index pointers
                      Index p = nnz_head;
                      for(Index k=0; k<m_outerSize.value(); ++k)
                      {
                        matrix.outerIndexPtr()[m_outerStart+k] = p;
                        p += tmp.innerVector(k).nonZeros();
                      }
                      std::ptrdiff_t offset = nnz - nnz_previous;
                      for(Index k = m_outerStart + m_outerSize.value(); k<=matrix.outerSize(); ++k)
                      {
                        matrix.outerIndexPtr()[k] += offset;
                      }
                
                      return *this;
                    }
                
                    inline SparseInnerVectorSet& operator=(const SparseInnerVectorSet& other)
                    {
                      return operator=<SparseInnerVectorSet>(other);
                    }
                
                    inline const Scalar* valuePtr() const
                    { return m_matrix.valuePtr() + m_matrix.outerIndexPtr()[m_outerStart]; }
                    inline Scalar* valuePtr()
                    { return m_matrix.const_cast_derived().valuePtr() + m_matrix.outerIndexPtr()[m_outerStart]; }
                
                    inline const Index* innerIndexPtr() const
                    { return m_matrix.innerIndexPtr() + m_matrix.outerIndexPtr()[m_outerStart]; }
                    inline Index* innerIndexPtr()
                    { return m_matrix.const_cast_derived().innerIndexPtr() + m_matrix.outerIndexPtr()[m_outerStart]; }
                
                    inline const Index* outerIndexPtr() const
                    { return m_matrix.outerIndexPtr() + m_outerStart; }
                    inline Index* outerIndexPtr()
                    { return m_matrix.const_cast_derived().outerIndexPtr() + m_outerStart; }
                
                    Index nonZeros() const
                    {
                      if(m_matrix.isCompressed())
                        return  std::size_t(m_matrix.outerIndexPtr()[m_outerStart+m_outerSize.value()])
                              - std::size_t(m_matrix.outerIndexPtr()[m_outerStart]);
                      else if(m_outerSize.value()==0)
                        return 0;
                      else
                        return Map<const Matrix<Index,Size,1> >(m_matrix.innerNonZeroPtr()+m_outerStart, m_outerSize.value()).sum();
                    }
                
                    const Scalar& lastCoeff() const
                    {
                      EIGEN_STATIC_ASSERT_VECTOR_ONLY(SparseInnerVectorSet);
                      eigen_assert(nonZeros()>0);
                      if(m_matrix.isCompressed())
                        return m_matrix.valuePtr()[m_matrix.outerIndexPtr()[m_outerStart+1]-1];
                      else
                        return m_matrix.valuePtr()[m_matrix.outerIndexPtr()[m_outerStart]+m_matrix.innerNonZeroPtr()[m_outerStart]-1];
                    }
                
                //     template<typename Sparse>
                //     inline SparseInnerVectorSet& operator=(const SparseMatrixBase<OtherDerived>& other)
                //     {
                //       return *this;
                //     }
                
                    EIGEN_STRONG_INLINE Index rows() const { return IsRowMajor ? m_outerSize.value() : m_matrix.rows(); }
                    EIGEN_STRONG_INLINE Index cols() const { return IsRowMajor ? m_matrix.cols() : m_outerSize.value(); }
                
                  protected:
                
                    typename MatrixType::Nested m_matrix;
                    Index m_outerStart;
                    const internal::variable_if_dynamic<Index, Size> m_outerSize;
                
                };
                
                //----------
                
                /** \returns the i-th row of the matrix \c *this. For row-major matrix only. */
                template<typename Derived>
                SparseInnerVectorSet<Derived,1> SparseMatrixBase<Derived>::row(Index i)
                {
                  EIGEN_STATIC_ASSERT(IsRowMajor,THIS_METHOD_IS_ONLY_FOR_ROW_MAJOR_MATRICES);
                  return innerVector(i);
                }
                
                /** \returns the i-th row of the matrix \c *this. For row-major matrix only.
                  * (read-only version) */
                template<typename Derived>
                const SparseInnerVectorSet<Derived,1> SparseMatrixBase<Derived>::row(Index i) const
                {
                  EIGEN_STATIC_ASSERT(IsRowMajor,THIS_METHOD_IS_ONLY_FOR_ROW_MAJOR_MATRICES);
                  return innerVector(i);
                }
                
                /** \returns the i-th column of the matrix \c *this. For column-major matrix only. */
                template<typename Derived>
                SparseInnerVectorSet<Derived,1> SparseMatrixBase<Derived>::col(Index i)
       38312 -> {
                  EIGEN_STATIC_ASSERT(!IsRowMajor,THIS_METHOD_IS_ONLY_FOR_COLUMN_MAJOR_MATRICES);
                  return innerVector(i);
                }
                
                /** \returns the i-th column of the matrix \c *this. For column-major matrix only.
                  * (read-only version) */
                template<typename Derived>
                const SparseInnerVectorSet<Derived,1> SparseMatrixBase<Derived>::col(Index i) const
                {
                  EIGEN_STATIC_ASSERT(!IsRowMajor,THIS_METHOD_IS_ONLY_FOR_COLUMN_MAJOR_MATRICES);
                  return innerVector(i);
                }
                
                /** \returns the \a outer -th column (resp. row) of the matrix \c *this if \c *this
                  * is col-major (resp. row-major).
                  */
                template<typename Derived>
                SparseInnerVectorSet<Derived,1> SparseMatrixBase<Derived>::innerVector(Index outer)
       38312 -> { return SparseInnerVectorSet<Derived,1>(derived(), outer); }
                
                /** \returns the \a outer -th column (resp. row) of the matrix \c *this if \c *this
                  * is col-major (resp. row-major). Read-only.
                  */
                template<typename Derived>
                const SparseInnerVectorSet<Derived,1> SparseMatrixBase<Derived>::innerVector(Index outer) const
                { return SparseInnerVectorSet<Derived,1>(derived(), outer); }
                
                /** \returns the i-th row of the matrix \c *this. For row-major matrix only. */
                template<typename Derived>
                SparseInnerVectorSet<Derived,Dynamic> SparseMatrixBase<Derived>::middleRows(Index start, Index size)
                {
                  EIGEN_STATIC_ASSERT(IsRowMajor,THIS_METHOD_IS_ONLY_FOR_ROW_MAJOR_MATRICES);
                  return innerVectors(start, size);
                }
                
                /** \returns the i-th row of the matrix \c *this. For row-major matrix only.
                  * (read-only version) */
                template<typename Derived>
                const SparseInnerVectorSet<Derived,Dynamic> SparseMatrixBase<Derived>::middleRows(Index start, Index size) const
                {
                  EIGEN_STATIC_ASSERT(IsRowMajor,THIS_METHOD_IS_ONLY_FOR_ROW_MAJOR_MATRICES);
                  return innerVectors(start, size);
                }
                
                /** \returns the i-th column of the matrix \c *this. For column-major matrix only. */
                template<typename Derived>
                SparseInnerVectorSet<Derived,Dynamic> SparseMatrixBase<Derived>::middleCols(Index start, Index size)
                {
                  EIGEN_STATIC_ASSERT(!IsRowMajor,THIS_METHOD_IS_ONLY_FOR_COLUMN_MAJOR_MATRICES);
                  return innerVectors(start, size);
                }
                
                /** \returns the i-th column of the matrix \c *this. For column-major matrix only.
                  * (read-only version) */
                template<typename Derived>
                const SparseInnerVectorSet<Derived,Dynamic> SparseMatrixBase<Derived>::middleCols(Index start, Index size) const
                {
                  EIGEN_STATIC_ASSERT(!IsRowMajor,THIS_METHOD_IS_ONLY_FOR_COLUMN_MAJOR_MATRICES);
                  return innerVectors(start, size);
                }
                
                
                
                /** \returns the \a outer -th column (resp. row) of the matrix \c *this if \c *this
                  * is col-major (resp. row-major).
                  */
                template<typename Derived>
                SparseInnerVectorSet<Derived,Dynamic> SparseMatrixBase<Derived>::innerVectors(Index outerStart, Index outerSize)
                { return SparseInnerVectorSet<Derived,Dynamic>(derived(), outerStart, outerSize); }
                
                /** \returns the \a outer -th column (resp. row) of the matrix \c *this if \c *this
                  * is col-major (resp. row-major). Read-only.
                  */
                template<typename Derived>
                const SparseInnerVectorSet<Derived,Dynamic> SparseMatrixBase<Derived>::innerVectors(Index outerStart, Index outerSize) const
                { return SparseInnerVectorSet<Derived,Dynamic>(derived(), outerStart, outerSize); }
                
                #endif // EIGEN_SPARSE_BLOCK_H


Top 10 Lines:

     Line      Count

      298      38312
      317      38312

Execution Summary:

        2   Executable lines in this file
        2   Lines executed
   100.00   Percent of the file executed

    76624   Total number of line executions
 38312.00   Average executions per line


*** File /cygdrive/c/Users/Paul/Documents/My Dropbox/Online Resources/My Homework/UBC 2011W/NSERC/matrix-factor/./Eigen/src/Core/util/Memory.h:
                // This file is part of Eigen, a lightweight C++ template library
                // for linear algebra.
                //
                // Copyright (C) 2008-2010 Gael Guennebaud <gael.guennebaud@inria.fr>
                // Copyright (C) 2008-2009 Benoit Jacob <jacob.benoit.1@gmail.com>
                // Copyright (C) 2009 Kenneth Riddile <kfriddile@yahoo.com>
                // Copyright (C) 2010 Hauke Heibel <hauke.heibel@gmail.com>
                // Copyright (C) 2010 Thomas Capricelli <orzel@freehackers.org>
                //
                // Eigen is free software; you can redistribute it and/or
                // modify it under the terms of the GNU Lesser General Public
                // License as published by the Free Software Foundation; either
                // version 3 of the License, or (at your option) any later version.
                //
                // Alternatively, you can redistribute it and/or
                // modify it under the terms of the GNU General Public License as
                // published by the Free Software Foundation; either version 2 of
                // the License, or (at your option) any later version.
                //
                // Eigen is distributed in the hope that it will be useful, but WITHOUT ANY
                // WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
                // FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License or the
                // GNU General Public License for more details.
                //
                // You should have received a copy of the GNU Lesser General Public
                // License and a copy of the GNU General Public License along with
                // Eigen. If not, see <http://www.gnu.org/licenses/>.
                
                
                /*****************************************************************************
                *** Platform checks for aligned malloc functions                           ***
                *****************************************************************************/
                
                #ifndef EIGEN_MEMORY_H
                #define EIGEN_MEMORY_H
                
                // On 64-bit systems, glibc's malloc returns 16-byte-aligned pointers, see:
                //   http://www.gnu.org/s/libc/manual/html_node/Aligned-Memory-Blocks.html
                // This is true at least since glibc 2.8.
                // This leaves the question how to detect 64-bit. According to this document,
                //   http://gcc.fyxm.net/summit/2003/Porting%20to%2064%20bit.pdf
                // page 114, "[The] LP64 model [...] is used by all 64-bit UNIX ports" so it's indeed
                // quite safe, at least within the context of glibc, to equate 64-bit with LP64.
                #if defined(__GLIBC__) && ((__GLIBC__>=2 && __GLIBC_MINOR__ >= 8) || __GLIBC__>2) \
                 && defined(__LP64__)
                  #define EIGEN_GLIBC_MALLOC_ALREADY_ALIGNED 1
                #else
                  #define EIGEN_GLIBC_MALLOC_ALREADY_ALIGNED 0
                #endif
                
                // FreeBSD 6 seems to have 16-byte aligned malloc
                //   See http://svn.freebsd.org/viewvc/base/stable/6/lib/libc/stdlib/malloc.c?view=markup
                // FreeBSD 7 seems to have 16-byte aligned malloc except on ARM and MIPS architectures
                //   See http://svn.freebsd.org/viewvc/base/stable/7/lib/libc/stdlib/malloc.c?view=markup
                #if defined(__FreeBSD__) && !defined(__arm__) && !defined(__mips__)
                  #define EIGEN_FREEBSD_MALLOC_ALREADY_ALIGNED 1
                #else
                  #define EIGEN_FREEBSD_MALLOC_ALREADY_ALIGNED 0
                #endif
                
                #if defined(__APPLE__) \
                 || defined(_WIN64) \
                 || EIGEN_GLIBC_MALLOC_ALREADY_ALIGNED \
                 || EIGEN_FREEBSD_MALLOC_ALREADY_ALIGNED
                  #define EIGEN_MALLOC_ALREADY_ALIGNED 1
                #else
                  #define EIGEN_MALLOC_ALREADY_ALIGNED 0
                #endif
                
                #if ((defined __QNXNTO__) || (defined _GNU_SOURCE) || ((defined _XOPEN_SOURCE) && (_XOPEN_SOURCE >= 600))) \
                 && (defined _POSIX_ADVISORY_INFO) && (_POSIX_ADVISORY_INFO > 0)
                  #define EIGEN_HAS_POSIX_MEMALIGN 1
                #else
                  #define EIGEN_HAS_POSIX_MEMALIGN 0
                #endif
                
                #ifdef EIGEN_VECTORIZE_SSE
                  #define EIGEN_HAS_MM_MALLOC 1
                #else
                  #define EIGEN_HAS_MM_MALLOC 0
                #endif
                
                namespace internal {
                
                inline void throw_std_bad_alloc()
                {
                  #ifdef EIGEN_EXCEPTIONS
                    throw std::bad_alloc();
                  #else
                    std::size_t huge = -1;
                    new int[huge];
                  #endif
                }
                
                /*****************************************************************************
                *** Implementation of handmade aligned functions                           ***
                *****************************************************************************/
                
                /* ----- Hand made implementations of aligned malloc/free and realloc ----- */
                
                /** \internal Like malloc, but the returned pointer is guaranteed to be 16-byte aligned.
                  * Fast, but wastes 16 additional bytes of memory. Does not throw any exception.
                  */
                inline void* handmade_aligned_malloc(size_t size)
                {
                  void *original = std::malloc(size+16);
                  if (original == 0) return 0;
                  void *aligned = reinterpret_cast<void*>((reinterpret_cast<size_t>(original) & ~(size_t(15))) + 16);
                  *(reinterpret_cast<void**>(aligned) - 1) = original;
                  return aligned;
                }
                
                /** \internal Frees memory allocated with handmade_aligned_malloc */
                inline void handmade_aligned_free(void *ptr)
                {
                  if (ptr) std::free(*(reinterpret_cast<void**>(ptr) - 1));
                }
                
                /** \internal
                  * \brief Reallocates aligned memory.
                  * Since we know that our handmade version is based on std::realloc
                  * we can use std::realloc to implement efficient reallocation.
                  */
                inline void* handmade_aligned_realloc(void* ptr, size_t size, size_t = 0)
                {
                  if (ptr == 0) return handmade_aligned_malloc(size);
                  void *original = *(reinterpret_cast<void**>(ptr) - 1);
                  original = std::realloc(original,size+16);
                  if (original == 0) return 0;
                  void *aligned = reinterpret_cast<void*>((reinterpret_cast<size_t>(original) & ~(size_t(15))) + 16);
                  *(reinterpret_cast<void**>(aligned) - 1) = original;
                  return aligned;
                }
                
                /*****************************************************************************
                *** Implementation of generic aligned realloc (when no realloc can be used)***
                *****************************************************************************/
                
                void* aligned_malloc(size_t size);
                void  aligned_free(void *ptr);
                
                /** \internal
                  * \brief Reallocates aligned memory.
                  * Allows reallocation with aligned ptr types. This implementation will
                  * always create a new memory chunk and copy the old data.
                  */
                inline void* generic_aligned_realloc(void* ptr, size_t size, size_t old_size)
                {
                  if (ptr==0)
                    return aligned_malloc(size);
                
                  if (size==0)
                  {
                    aligned_free(ptr);
                    return 0;
                  }
                
                  void* newptr = aligned_malloc(size);
                  if (newptr == 0)
                  {
                    #ifdef EIGEN_HAS_ERRNO
                    errno = ENOMEM; // according to the standard
                    #endif
                    return 0;
                  }
                
                  if (ptr != 0)
                  {
                    std::memcpy(newptr, ptr, (std::min)(size,old_size));
                    aligned_free(ptr);
                  }
                
                  return newptr;
                }
                
                /*****************************************************************************
                *** Implementation of portable aligned versions of malloc/free/realloc     ***
                *****************************************************************************/
                
                #ifdef EIGEN_NO_MALLOC
                inline void check_that_malloc_is_allowed()
                {
                  eigen_assert(false && "heap allocation is forbidden (EIGEN_NO_MALLOC is defined)");
                }
                #elif defined EIGEN_RUNTIME_NO_MALLOC
                inline bool is_malloc_allowed_impl(bool update, bool new_value = false)
                {
                  static bool value = true;
                  if (update == 1)
                    value = new_value;
                  return value;
                }
                inline bool is_malloc_allowed() { return is_malloc_allowed_impl(false); }
                inline bool set_is_malloc_allowed(bool new_value) { return is_malloc_allowed_impl(true, new_value); }
                inline void check_that_malloc_is_allowed()
                {
                  eigen_assert(is_malloc_allowed() && "heap allocation is forbidden (EIGEN_RUNTIME_NO_MALLOC is defined and g_is_malloc_allowed is false)");
                }
                #else 
                inline void check_that_malloc_is_allowed()
                {}
                #endif
                
                /** \internal Allocates \a size bytes. The returned pointer is guaranteed to have 16 bytes alignment.
                  * On allocation error, the returned pointer is null, and std::bad_alloc is thrown.
                  */
                inline void* aligned_malloc(size_t size)
                {
                  check_that_malloc_is_allowed();
                
                  void *result;
                  #if !EIGEN_ALIGN
                    result = std::malloc(size);
                  #elif EIGEN_MALLOC_ALREADY_ALIGNED
                    result = std::malloc(size);
                  #elif EIGEN_HAS_POSIX_MEMALIGN
                    if(posix_memalign(&result, 16, size)) result = 0;
                  #elif EIGEN_HAS_MM_MALLOC
                    result = _mm_malloc(size, 16);
                  #elif (defined _MSC_VER)
                    result = _aligned_malloc(size, 16);
                  #else
                    result = handmade_aligned_malloc(size);
                  #endif
                
                  if(!result && size)
                    throw_std_bad_alloc();
                
                  return result;
                }
                
                /** \internal Frees memory allocated with aligned_malloc. */
                inline void aligned_free(void *ptr)
                {
                  #if !EIGEN_ALIGN
                    std::free(ptr);
                  #elif EIGEN_MALLOC_ALREADY_ALIGNED
                    std::free(ptr);
                  #elif EIGEN_HAS_POSIX_MEMALIGN
                    std::free(ptr);
                  #elif EIGEN_HAS_MM_MALLOC
                    _mm_free(ptr);
                  #elif defined(_MSC_VER)
                    _aligned_free(ptr);
                  #else
                    handmade_aligned_free(ptr);
                  #endif
                }
                
                /**
                * \internal
                * \brief Reallocates an aligned block of memory.
                * \throws std::bad_alloc on allocation failure
                **/
                inline void* aligned_realloc(void *ptr, size_t new_size, size_t old_size)
                {
                  EIGEN_UNUSED_VARIABLE(old_size);
                
                  void *result;
                #if !EIGEN_ALIGN
                  result = std::realloc(ptr,new_size);
                #elif EIGEN_MALLOC_ALREADY_ALIGNED
                  result = std::realloc(ptr,new_size);
                #elif EIGEN_HAS_POSIX_MEMALIGN
                  result = generic_aligned_realloc(ptr,new_size,old_size);
                #elif EIGEN_HAS_MM_MALLOC
                  // The defined(_mm_free) is just here to verify that this MSVC version
                  // implements _mm_malloc/_mm_free based on the corresponding _aligned_
                  // functions. This may not always be the case and we just try to be safe.
                  #if defined(_MSC_VER) && defined(_mm_free)
                    result = _aligned_realloc(ptr,new_size,16);
                  #else
                    result = generic_aligned_realloc(ptr,new_size,old_size);
                  #endif
                #elif defined(_MSC_VER)
                  result = _aligned_realloc(ptr,new_size,16);
                #else
                  result = handmade_aligned_realloc(ptr,new_size,old_size);
                #endif
                
                  if (!result && new_size)
                    throw_std_bad_alloc();
                
                  return result;
                }
                
                /*****************************************************************************
                *** Implementation of conditionally aligned functions                      ***
                *****************************************************************************/
                
                /** \internal Allocates \a size bytes. If Align is true, then the returned ptr is 16-byte-aligned.
                  * On allocation error, the returned pointer is null, and a std::bad_alloc is thrown.
                  */
                template<bool Align> inline void* conditional_aligned_malloc(size_t size)
                {
                  return aligned_malloc(size);
                }
                
                template<> inline void* conditional_aligned_malloc<false>(size_t size)
                {
                  check_that_malloc_is_allowed();
                
                  void *result = std::malloc(size);
                  if(!result && size)
                    throw_std_bad_alloc();
                  return result;
                }
                
                /** \internal Frees memory allocated with conditional_aligned_malloc */
                template<bool Align> inline void conditional_aligned_free(void *ptr)
                {
                  aligned_free(ptr);
                }
                
                template<> inline void conditional_aligned_free<false>(void *ptr)
                {
                  std::free(ptr);
                }
                
                template<bool Align> inline void* conditional_aligned_realloc(void* ptr, size_t new_size, size_t old_size)
                {
                  return aligned_realloc(ptr, new_size, old_size);
                }
                
                template<> inline void* conditional_aligned_realloc<false>(void* ptr, size_t new_size, size_t)
                {
                  return std::realloc(ptr, new_size);
                }
                
                /*****************************************************************************
                *** Construction/destruction of array elements                             ***
                *****************************************************************************/
                
                /** \internal Constructs the elements of an array.
                  * The \a size parameter tells on how many objects to call the constructor of T.
                  */
                template<typename T> inline T* construct_elements_of_array(T *ptr, size_t size)
                {
                  for (size_t i=0; i < size; ++i) ::new (ptr + i) T;
                  return ptr;
                }
                
                /** \internal Destructs the elements of an array.
                  * The \a size parameters tells on how many objects to call the destructor of T.
                  */
                template<typename T> inline void destruct_elements_of_array(T *ptr, size_t size)
                {
                  // always destruct an array starting from the end.
                  if(ptr)
                    while(size) ptr[--size].~T();
                }
                
                /*****************************************************************************
                *** Implementation of aligned new/delete-like functions                    ***
                *****************************************************************************/
                
                template<typename T>
                EIGEN_ALWAYS_INLINE void check_size_for_overflow(size_t size)
                {
                  if(size > size_t(-1) / sizeof(T))
                    throw_std_bad_alloc();
                }
                
                /** \internal Allocates \a size objects of type T. The returned pointer is guaranteed to have 16 bytes alignment.
                  * On allocation error, the returned pointer is undefined, but a std::bad_alloc is thrown.
                  * The default constructor of T is called.
                  */
                template<typename T> inline T* aligned_new(size_t size)
                {
                  check_size_for_overflow<T>(size);
                  T *result = reinterpret_cast<T*>(aligned_malloc(sizeof(T)*size));
                  return construct_elements_of_array(result, size);
                }
                
                template<typename T, bool Align> inline T* conditional_aligned_new(size_t size)
                {
                  check_size_for_overflow<T>(size);
                  T *result = reinterpret_cast<T*>(conditional_aligned_malloc<Align>(sizeof(T)*size));
                  return construct_elements_of_array(result, size);
                }
                
                /** \internal Deletes objects constructed with aligned_new
                  * The \a size parameters tells on how many objects to call the destructor of T.
                  */
                template<typename T> inline void aligned_delete(T *ptr, size_t size)
                {
                  destruct_elements_of_array<T>(ptr, size);
                  aligned_free(ptr);
                }
                
                /** \internal Deletes objects constructed with conditional_aligned_new
                  * The \a size parameters tells on how many objects to call the destructor of T.
                  */
                template<typename T, bool Align> inline void conditional_aligned_delete(T *ptr, size_t size)
                {
                  destruct_elements_of_array<T>(ptr, size);
                  conditional_aligned_free<Align>(ptr);
                }
                
                template<typename T, bool Align> inline T* conditional_aligned_realloc_new(T* pts, size_t new_size, size_t old_size)
                {
                  check_size_for_overflow<T>(new_size);
                  check_size_for_overflow<T>(old_size);
                  if(new_size < old_size)
                    destruct_elements_of_array(pts+new_size, old_size-new_size);
                  T *result = reinterpret_cast<T*>(conditional_aligned_realloc<Align>(reinterpret_cast<void*>(pts), sizeof(T)*new_size, sizeof(T)*old_size));
                  if(new_size > old_size)
                    construct_elements_of_array(result+old_size, new_size-old_size);
                  return result;
                }
                
                
                template<typename T, bool Align> inline T* conditional_aligned_new_auto(size_t size)
                {
                  check_size_for_overflow<T>(size);
                  T *result = reinterpret_cast<T*>(conditional_aligned_malloc<Align>(sizeof(T)*size));
                  if(NumTraits<T>::RequireInitialization)
                    construct_elements_of_array(result, size);
                  return result;
                }
                
                template<typename T, bool Align> inline T* conditional_aligned_realloc_new_auto(T* pts, size_t new_size, size_t old_size)
                {
                  check_size_for_overflow<T>(new_size);
                  check_size_for_overflow<T>(old_size);
                  if(NumTraits<T>::RequireInitialization && (new_size < old_size))
                    destruct_elements_of_array(pts+new_size, old_size-new_size);
                  T *result = reinterpret_cast<T*>(conditional_aligned_realloc<Align>(reinterpret_cast<void*>(pts), sizeof(T)*new_size, sizeof(T)*old_size));
                  if(NumTraits<T>::RequireInitialization && (new_size > old_size))
                    construct_elements_of_array(result+old_size, new_size-old_size);
                  return result;
                }
                
                template<typename T, bool Align> inline void conditional_aligned_delete_auto(T *ptr, size_t size)
                {
                  if(NumTraits<T>::RequireInitialization)
                    destruct_elements_of_array<T>(ptr, size);
                  conditional_aligned_free<Align>(ptr);
                }
                
                /****************************************************************************/
                
                /** \internal Returns the index of the first element of the array that is well aligned for vectorization.
                  *
                  * \param array the address of the start of the array
                  * \param size the size of the array
                  *
                  * \note If no element of the array is well aligned, the size of the array is returned. Typically,
                  * for example with SSE, "well aligned" means 16-byte-aligned. If vectorization is disabled or if the
                  * packet size for the given scalar type is 1, then everything is considered well-aligned.
                  *
                  * \note If the scalar type is vectorizable, we rely on the following assumptions: sizeof(Scalar) is a
                  * power of 2, the packet size in bytes is also a power of 2, and is a multiple of sizeof(Scalar). On the
                  * other hand, we do not assume that the array address is a multiple of sizeof(Scalar), as that fails for
                  * example with Scalar=double on certain 32-bit platforms, see bug #79.
                  *
                  * There is also the variant first_aligned(const MatrixBase&) defined in DenseCoeffsBase.h.
                  */
                template<typename Scalar, typename Index>
                static inline Index first_aligned(const Scalar* array, Index size)
                {
                  typedef typename packet_traits<Scalar>::type Packet;
                  enum { PacketSize = packet_traits<Scalar>::size,
                         PacketAlignedMask = PacketSize-1
                  };
                
                  if(PacketSize==1)
                  {
                    // Either there is no vectorization, or a packet consists of exactly 1 scalar so that all elements
                    // of the array have the same alignment.
                    return 0;
                  }
                  else if(size_t(array) & (sizeof(Scalar)-1))
                  {
                    // There is vectorization for this scalar type, but the array is not aligned to the size of a single scalar.
                    // Consequently, no element of the array is well aligned.
                    return size;
                  }
                  else
                  {
                    return std::min<Index>( (PacketSize - (Index((size_t(array)/sizeof(Scalar))) & PacketAlignedMask))
                                           & PacketAlignedMask, size);
                  }
                }
                
                
                // std::copy is much slower than memcpy, so let's introduce a smart_copy which
                // use memcpy on trivial types, i.e., on types that does not require an initialization ctor.
                template<typename T, bool UseMemcpy> struct smart_copy_helper;
                
                template<typename T> void smart_copy(const T* start, const T* end, T* target)
      246802 -> {
                  smart_copy_helper<T,!NumTraits<T>::RequireInitialization>::run(start, end, target);
                }
                
                template<typename T> struct smart_copy_helper<T,true> {
                  static inline void run(const T* start, const T* end, T* target)
                  { memcpy(target, start, std::ptrdiff_t(end)-std::ptrdiff_t(start)); }
                };
                
                template<typename T> struct smart_copy_helper<T,false> {
                  static inline void run(const T* start, const T* end, T* target)
                  { std::copy(start, end, target); }
                };
                
                
                } // end namespace internal
                
                /*****************************************************************************
                *** Implementation of runtime stack allocation (falling back to malloc)    ***
                *****************************************************************************/
                
                // you can overwrite Eigen's default behavior regarding alloca by defining EIGEN_ALLOCA
                // to the appropriate stack allocation function
                #ifndef EIGEN_ALLOCA
                  #if (defined __linux__)
                    #define EIGEN_ALLOCA alloca
                  #elif defined(_MSC_VER)
                    #define EIGEN_ALLOCA _alloca
                  #endif
                #endif
                
                namespace internal {
                
                // This helper class construct the allocated memory, and takes care of destructing and freeing the handled data
                // at destruction time. In practice this helper class is mainly useful to avoid memory leak in case of exceptions.
                template<typename T> class aligned_stack_memory_handler
                {
                  public:
                    /* Creates a stack_memory_handler responsible for the buffer \a ptr of size \a size.
                     * Note that \a ptr can be 0 regardless of the other parameters.
                     * This constructor takes care of constructing/initializing the elements of the buffer if required by the scalar type T (see NumTraits<T>::RequireInitialization).
                     * In this case, the buffer elements will also be destructed when this handler will be destructed.
                     * Finally, if \a dealloc is true, then the pointer \a ptr is freed.
                     **/
                    aligned_stack_memory_handler(T* ptr, size_t size, bool dealloc)
                      : m_ptr(ptr), m_size(size), m_deallocate(dealloc)
                    {
                      if(NumTraits<T>::RequireInitialization && m_ptr)
                        Eigen::internal::construct_elements_of_array(m_ptr, size);
                    }
                    ~aligned_stack_memory_handler()
                    {
                      if(NumTraits<T>::RequireInitialization && m_ptr)
                        Eigen::internal::destruct_elements_of_array<T>(m_ptr, m_size);
                      if(m_deallocate)
                        Eigen::internal::aligned_free(m_ptr);
                    }
                  protected:
                    T* m_ptr;
                    size_t m_size;
                    bool m_deallocate;
                };
                
                }
                
                /** \internal
                  * Declares, allocates and construct an aligned buffer named NAME of SIZE elements of type TYPE on the stack
                  * if SIZE is smaller than EIGEN_STACK_ALLOCATION_LIMIT, and if stack allocation is supported by the platform
                  * (currently, this is Linux and Visual Studio only). Otherwise the memory is allocated on the heap.
                  * The allocated buffer is automatically deleted when exiting the scope of this declaration.
                  * If BUFFER is non null, then the declared variable is simply an alias for BUFFER, and no allocation/deletion occurs.
                  * Here is an example:
                  * \code
                  * {
                  *   ei_declare_aligned_stack_constructed_variable(float,data,size,0);
                  *   // use data[0] to data[size-1]
                  * }
                  * \endcode
                  * The underlying stack allocation function can controlled with the EIGEN_ALLOCA preprocessor token.
                  */
                #ifdef EIGEN_ALLOCA
                
                  #ifdef __arm__
                    #define EIGEN_ALIGNED_ALLOCA(SIZE) reinterpret_cast<void*>((reinterpret_cast<size_t>(EIGEN_ALLOCA(SIZE+16)) & ~(size_t(15))) + 16)
                  #else
                    #define EIGEN_ALIGNED_ALLOCA EIGEN_ALLOCA
                  #endif
                
                  #define ei_declare_aligned_stack_constructed_variable(TYPE,NAME,SIZE,BUFFER) \
                    Eigen::internal::check_size_for_overflow<TYPE>(SIZE); \
                    TYPE* NAME = (BUFFER)!=0 ? (BUFFER) \
                               : reinterpret_cast<TYPE*>( \
                                      (sizeof(TYPE)*SIZE<=EIGEN_STACK_ALLOCATION_LIMIT) ? EIGEN_ALIGNED_ALLOCA(sizeof(TYPE)*SIZE) \
                                    : Eigen::internal::aligned_malloc(sizeof(TYPE)*SIZE) );  \
                    Eigen::internal::aligned_stack_memory_handler<TYPE> EIGEN_CAT(NAME,_stack_memory_destructor)((BUFFER)==0 ? NAME : 0,SIZE,sizeof(TYPE)*SIZE>EIGEN_STACK_ALLOCATION_LIMIT)
                
                #else
                
                  #define ei_declare_aligned_stack_constructed_variable(TYPE,NAME,SIZE,BUFFER) \
                    Eigen::internal::check_size_for_overflow<TYPE>(SIZE); \
                    TYPE* NAME = (BUFFER)!=0 ? BUFFER : reinterpret_cast<TYPE*>(Eigen::internal::aligned_malloc(sizeof(TYPE)*SIZE));    \
                    Eigen::internal::aligned_stack_memory_handler<TYPE> EIGEN_CAT(NAME,_stack_memory_destructor)((BUFFER)==0 ? NAME : 0,SIZE,true)
                    
                #endif
                
                
                /*****************************************************************************
                *** Implementation of EIGEN_MAKE_ALIGNED_OPERATOR_NEW [_IF]                ***
                *****************************************************************************/
                
                #if EIGEN_ALIGN
                  #ifdef EIGEN_EXCEPTIONS
                    #define EIGEN_MAKE_ALIGNED_OPERATOR_NEW_NOTHROW(NeedsToAlign) \
                      void* operator new(size_t size, const std::nothrow_t&) throw() { \
                        try { return Eigen::internal::conditional_aligned_malloc<NeedsToAlign>(size); } \
                        catch (...) { return 0; } \
                        return 0; \
                      }
                  #else
                    #define EIGEN_MAKE_ALIGNED_OPERATOR_NEW_NOTHROW(NeedsToAlign) \
                      void* operator new(size_t size, const std::nothrow_t&) throw() { \
                        return Eigen::internal::conditional_aligned_malloc<NeedsToAlign>(size); \
                      }
                  #endif
                
                  #define EIGEN_MAKE_ALIGNED_OPERATOR_NEW_IF(NeedsToAlign) \
                      void *operator new(size_t size) { \
                        return Eigen::internal::conditional_aligned_malloc<NeedsToAlign>(size); \
                      } \
                      void *operator new[](size_t size) { \
                        return Eigen::internal::conditional_aligned_malloc<NeedsToAlign>(size); \
                      } \
                      void operator delete(void * ptr) throw() { Eigen::internal::conditional_aligned_free<NeedsToAlign>(ptr); } \
                      void operator delete[](void * ptr) throw() { Eigen::internal::conditional_aligned_free<NeedsToAlign>(ptr); } \
                      /* in-place new and delete. since (at least afaik) there is no actual   */ \
                      /* memory allocated we can safely let the default implementation handle */ \
                      /* this particular case. */ \
                      static void *operator new(size_t size, void *ptr) { return ::operator new(size,ptr); } \
                      void operator delete(void * memory, void *ptr) throw() { return ::operator delete(memory,ptr); } \
                      /* nothrow-new (returns zero instead of std::bad_alloc) */ \
                      EIGEN_MAKE_ALIGNED_OPERATOR_NEW_NOTHROW(NeedsToAlign) \
                      void operator delete(void *ptr, const std::nothrow_t&) throw() { \
                        Eigen::internal::conditional_aligned_free<NeedsToAlign>(ptr); \
                      } \
                      typedef void eigen_aligned_operator_new_marker_type;
                #else
                  #define EIGEN_MAKE_ALIGNED_OPERATOR_NEW_IF(NeedsToAlign)
                #endif
                
                #define EIGEN_MAKE_ALIGNED_OPERATOR_NEW EIGEN_MAKE_ALIGNED_OPERATOR_NEW_IF(true)
                #define EIGEN_MAKE_ALIGNED_OPERATOR_NEW_IF_VECTORIZABLE_FIXED_SIZE(Scalar,Size) \
                  EIGEN_MAKE_ALIGNED_OPERATOR_NEW_IF(bool(((Size)!=Eigen::Dynamic) && ((sizeof(Scalar)*(Size))%16==0)))
                
                /****************************************************************************/
                
                /** \class aligned_allocator
                * \ingroup Core_Module
                *
                * \brief STL compatible allocator to use with with 16 byte aligned types
                *
                * Example:
                * \code
                * // Matrix4f requires 16 bytes alignment:
                * std::map< int, Matrix4f, std::less<int>, 
                *           aligned_allocator<std::pair<const int, Matrix4f> > > my_map_mat4;
                * // Vector3f does not require 16 bytes alignment, no need to use Eigen's allocator:
                * std::map< int, Vector3f > my_map_vec3;
                * \endcode
                *
                * \sa \ref TopicStlContainers.
                */
                template<class T>
                class aligned_allocator
                {
                public:
                    typedef size_t    size_type;
                    typedef std::ptrdiff_t difference_type;
                    typedef T*        pointer;
                    typedef const T*  const_pointer;
                    typedef T&        reference;
                    typedef const T&  const_reference;
                    typedef T         value_type;
                
                    template<class U>
                    struct rebind
                    {
                        typedef aligned_allocator<U> other;
                    };
                
                    pointer address( reference value ) const
                    {
                        return &value;
                    }
                
                    const_pointer address( const_reference value ) const
                    {
                        return &value;
                    }
                
                    aligned_allocator()
                    {
                    }
                
                    aligned_allocator( const aligned_allocator& )
                    {
                    }
                
                    template<class U>
                    aligned_allocator( const aligned_allocator<U>& )
                    {
                    }
                
                    ~aligned_allocator()
                    {
                    }
                
                    size_type max_size() const
                    {
                        return (std::numeric_limits<size_type>::max)();
                    }
                
                    pointer allocate( size_type num, const void* hint = 0 )
                    {
                        EIGEN_UNUSED_VARIABLE(hint);
                        internal::check_size_for_overflow<T>(num);
                        return static_cast<pointer>( internal::aligned_malloc( num * sizeof(T) ) );
                    }
                
                    void construct( pointer p, const T& value )
                    {
                        ::new( p ) T( value );
                    }
                
                    void destroy( pointer p )
                    {
                        p->~T();
                    }
                
                    void deallocate( pointer p, size_type /*num*/ )
                    {
                        internal::aligned_free( p );
                    }
                
                    bool operator!=(const aligned_allocator<T>& ) const
                    { return false; }
                
                    bool operator==(const aligned_allocator<T>& ) const
                    { return true; }
                };
                
                //---------- Cache sizes ----------
                
                #if !defined(EIGEN_NO_CPUID)
                #  if defined(__GNUC__) && ( defined(__i386__) || defined(__x86_64__) )
                #    if defined(__PIC__) && defined(__i386__)
                       // Case for x86 with PIC
                #      define EIGEN_CPUID(abcd,func,id) \
                         __asm__ __volatile__ ("xchgl %%ebx, %%esi;cpuid; xchgl %%ebx,%%esi": "=a" (abcd[0]), "=S" (abcd[1]), "=c" (abcd[2]), "=d" (abcd[3]) : "a" (func), "c" (id));
                #    else
                       // Case for x86_64 or x86 w/o PIC
                #      define EIGEN_CPUID(abcd,func,id) \
                         __asm__ __volatile__ ("cpuid": "=a" (abcd[0]), "=b" (abcd[1]), "=c" (abcd[2]), "=d" (abcd[3]) : "a" (func), "c" (id) );
                #    endif
                #  elif defined(_MSC_VER)
                #    if (_MSC_VER > 1500)
                #      define EIGEN_CPUID(abcd,func,id) __cpuidex((int*)abcd,func,id)
                #    endif
                #  endif
                #endif
                
                namespace internal {
                
                #ifdef EIGEN_CPUID
                
                inline bool cpuid_is_vendor(int abcd[4], const char* vendor)
                {
                  return abcd[1]==(reinterpret_cast<const int*>(vendor))[0] && abcd[3]==(reinterpret_cast<const int*>(vendor))[1] && abcd[2]==(reinterpret_cast<const int*>(vendor))[2];
                }
                
                inline void queryCacheSizes_intel_direct(int& l1, int& l2, int& l3)
                {
                  int abcd[4];
                  l1 = l2 = l3 = 0;
                  int cache_id = 0;
                  int cache_type = 0;
                  do {
                    abcd[0] = abcd[1] = abcd[2] = abcd[3] = 0;
                    EIGEN_CPUID(abcd,0x4,cache_id);
                    cache_type  = (abcd[0] & 0x0F) >> 0;
                    if(cache_type==1||cache_type==3) // data or unified cache
                    {
                      int cache_level = (abcd[0] & 0xE0) >> 5;  // A[7:5]
                      int ways        = (abcd[1] & 0xFFC00000) >> 22; // B[31:22]
                      int partitions  = (abcd[1] & 0x003FF000) >> 12; // B[21:12]
                      int line_size   = (abcd[1] & 0x00000FFF) >>  0; // B[11:0]
                      int sets        = (abcd[2]);                    // C[31:0]
                
                      int cache_size = (ways+1) * (partitions+1) * (line_size+1) * (sets+1);
                
                      switch(cache_level)
                      {
                        case 1: l1 = cache_size; break;
                        case 2: l2 = cache_size; break;
                        case 3: l3 = cache_size; break;
                        default: break;
                      }
                    }
                    cache_id++;
                  } while(cache_type>0 && cache_id<16);
                }
                
                inline void queryCacheSizes_intel_codes(int& l1, int& l2, int& l3)
                {
                  int abcd[4];
                  abcd[0] = abcd[1] = abcd[2] = abcd[3] = 0;
                  l1 = l2 = l3 = 0;
                  EIGEN_CPUID(abcd,0x00000002,0);
                  unsigned char * bytes = reinterpret_cast<unsigned char *>(abcd)+2;
                  bool check_for_p2_core2 = false;
                  for(int i=0; i<14; ++i)
                  {
                    switch(bytes[i])
                    {
                      case 0x0A: l1 = 8; break;   // 0Ah   data L1 cache, 8 KB, 2 ways, 32 byte lines
                      case 0x0C: l1 = 16; break;  // 0Ch   data L1 cache, 16 KB, 4 ways, 32 byte lines
                      case 0x0E: l1 = 24; break;  // 0Eh   data L1 cache, 24 KB, 6 ways, 64 byte lines
                      case 0x10: l1 = 16; break;  // 10h   data L1 cache, 16 KB, 4 ways, 32 byte lines (IA-64)
                      case 0x15: l1 = 16; break;  // 15h   code L1 cache, 16 KB, 4 ways, 32 byte lines (IA-64)
                      case 0x2C: l1 = 32; break;  // 2Ch   data L1 cache, 32 KB, 8 ways, 64 byte lines
                      case 0x30: l1 = 32; break;  // 30h   code L1 cache, 32 KB, 8 ways, 64 byte lines
                      case 0x60: l1 = 16; break;  // 60h   data L1 cache, 16 KB, 8 ways, 64 byte lines, sectored
                      case 0x66: l1 = 8; break;   // 66h   data L1 cache, 8 KB, 4 ways, 64 byte lines, sectored
                      case 0x67: l1 = 16; break;  // 67h   data L1 cache, 16 KB, 4 ways, 64 byte lines, sectored
                      case 0x68: l1 = 32; break;  // 68h   data L1 cache, 32 KB, 4 ways, 64 byte lines, sectored
                      case 0x1A: l2 = 96; break;   // code and data L2 cache, 96 KB, 6 ways, 64 byte lines (IA-64)
                      case 0x22: l3 = 512; break;   // code and data L3 cache, 512 KB, 4 ways (!), 64 byte lines, dual-sectored
                      case 0x23: l3 = 1024; break;   // code and data L3 cache, 1024 KB, 8 ways, 64 byte lines, dual-sectored
                      case 0x25: l3 = 2048; break;   // code and data L3 cache, 2048 KB, 8 ways, 64 byte lines, dual-sectored
                      case 0x29: l3 = 4096; break;   // code and data L3 cache, 4096 KB, 8 ways, 64 byte lines, dual-sectored
                      case 0x39: l2 = 128; break;   // code and data L2 cache, 128 KB, 4 ways, 64 byte lines, sectored
                      case 0x3A: l2 = 192; break;   // code and data L2 cache, 192 KB, 6 ways, 64 byte lines, sectored
                      case 0x3B: l2 = 128; break;   // code and data L2 cache, 128 KB, 2 ways, 64 byte lines, sectored
                      case 0x3C: l2 = 256; break;   // code and data L2 cache, 256 KB, 4 ways, 64 byte lines, sectored
                      case 0x3D: l2 = 384; break;   // code and data L2 cache, 384 KB, 6 ways, 64 byte lines, sectored
                      case 0x3E: l2 = 512; break;   // code and data L2 cache, 512 KB, 4 ways, 64 byte lines, sectored
                      case 0x40: l2 = 0; break;   // no integrated L2 cache (P6 core) or L3 cache (P4 core)
                      case 0x41: l2 = 128; break;   // code and data L2 cache, 128 KB, 4 ways, 32 byte lines
                      case 0x42: l2 = 256; break;   // code and data L2 cache, 256 KB, 4 ways, 32 byte lines
                      case 0x43: l2 = 512; break;   // code and data L2 cache, 512 KB, 4 ways, 32 byte lines
                      case 0x44: l2 = 1024; break;   // code and data L2 cache, 1024 KB, 4 ways, 32 byte lines
                      case 0x45: l2 = 2048; break;   // code and data L2 cache, 2048 KB, 4 ways, 32 byte lines
                      case 0x46: l3 = 4096; break;   // code and data L3 cache, 4096 KB, 4 ways, 64 byte lines
                      case 0x47: l3 = 8192; break;   // code and data L3 cache, 8192 KB, 8 ways, 64 byte lines
                      case 0x48: l2 = 3072; break;   // code and data L2 cache, 3072 KB, 12 ways, 64 byte lines
                      case 0x49: if(l2!=0) l3 = 4096; else {check_for_p2_core2=true; l3 = l2 = 4096;} break;// code and data L3 cache, 4096 KB, 16 ways, 64 byte lines (P4) or L2 for core2
                      case 0x4A: l3 = 6144; break;   // code and data L3 cache, 6144 KB, 12 ways, 64 byte lines
                      case 0x4B: l3 = 8192; break;   // code and data L3 cache, 8192 KB, 16 ways, 64 byte lines
                      case 0x4C: l3 = 12288; break;   // code and data L3 cache, 12288 KB, 12 ways, 64 byte lines
                      case 0x4D: l3 = 16384; break;   // code and data L3 cache, 16384 KB, 16 ways, 64 byte lines
                      case 0x4E: l2 = 6144; break;   // code and data L2 cache, 6144 KB, 24 ways, 64 byte lines
                      case 0x78: l2 = 1024; break;   // code and data L2 cache, 1024 KB, 4 ways, 64 byte lines
                      case 0x79: l2 = 128; break;   // code and data L2 cache, 128 KB, 8 ways, 64 byte lines, dual-sectored
                      case 0x7A: l2 = 256; break;   // code and data L2 cache, 256 KB, 8 ways, 64 byte lines, dual-sectored
                      case 0x7B: l2 = 512; break;   // code and data L2 cache, 512 KB, 8 ways, 64 byte lines, dual-sectored
                      case 0x7C: l2 = 1024; break;   // code and data L2 cache, 1024 KB, 8 ways, 64 byte lines, dual-sectored
                      case 0x7D: l2 = 2048; break;   // code and data L2 cache, 2048 KB, 8 ways, 64 byte lines
                      case 0x7E: l2 = 256; break;   // code and data L2 cache, 256 KB, 8 ways, 128 byte lines, sect. (IA-64)
                      case 0x7F: l2 = 512; break;   // code and data L2 cache, 512 KB, 2 ways, 64 byte lines
                      case 0x80: l2 = 512; break;   // code and data L2 cache, 512 KB, 8 ways, 64 byte lines
                      case 0x81: l2 = 128; break;   // code and data L2 cache, 128 KB, 8 ways, 32 byte lines
                      case 0x82: l2 = 256; break;   // code and data L2 cache, 256 KB, 8 ways, 32 byte lines
                      case 0x83: l2 = 512; break;   // code and data L2 cache, 512 KB, 8 ways, 32 byte lines
                      case 0x84: l2 = 1024; break;   // code and data L2 cache, 1024 KB, 8 ways, 32 byte lines
                      case 0x85: l2 = 2048; break;   // code and data L2 cache, 2048 KB, 8 ways, 32 byte lines
                      case 0x86: l2 = 512; break;   // code and data L2 cache, 512 KB, 4 ways, 64 byte lines
                      case 0x87: l2 = 1024; break;   // code and data L2 cache, 1024 KB, 8 ways, 64 byte lines
                      case 0x88: l3 = 2048; break;   // code and data L3 cache, 2048 KB, 4 ways, 64 byte lines (IA-64)
                      case 0x89: l3 = 4096; break;   // code and data L3 cache, 4096 KB, 4 ways, 64 byte lines (IA-64)
                      case 0x8A: l3 = 8192; break;   // code and data L3 cache, 8192 KB, 4 ways, 64 byte lines (IA-64)
                      case 0x8D: l3 = 3072; break;   // code and data L3 cache, 3072 KB, 12 ways, 128 byte lines (IA-64)
                
                      default: break;
                    }
                  }
                  if(check_for_p2_core2 && l2 == l3)
                    l3 = 0;
                  l1 *= 1024;
                  l2 *= 1024;
                  l3 *= 1024;
                }
                
                inline void queryCacheSizes_intel(int& l1, int& l2, int& l3, int max_std_funcs)
                {
                  if(max_std_funcs>=4)
                    queryCacheSizes_intel_direct(l1,l2,l3);
                  else
                    queryCacheSizes_intel_codes(l1,l2,l3);
                }
                
                inline void queryCacheSizes_amd(int& l1, int& l2, int& l3)
                {
                  int abcd[4];
                  abcd[0] = abcd[1] = abcd[2] = abcd[3] = 0;
                  EIGEN_CPUID(abcd,0x80000005,0);
                  l1 = (abcd[2] >> 24) * 1024; // C[31:24] = L1 size in KB
                  abcd[0] = abcd[1] = abcd[2] = abcd[3] = 0;
                  EIGEN_CPUID(abcd,0x80000006,0);
                  l2 = (abcd[2] >> 16) * 1024; // C[31;16] = l2 cache size in KB
                  l3 = ((abcd[3] & 0xFFFC000) >> 18) * 512 * 1024; // D[31;18] = l3 cache size in 512KB
                }
                #endif
                
                /** \internal
                 * Queries and returns the cache sizes in Bytes of the L1, L2, and L3 data caches respectively */
                inline void queryCacheSizes(int& l1, int& l2, int& l3)
                {
                  #ifdef EIGEN_CPUID
                  int abcd[4];
                
                  // identify the CPU vendor
                  EIGEN_CPUID(abcd,0x0,0);
                  int max_std_funcs = abcd[1];
                  if(cpuid_is_vendor(abcd,"GenuineIntel"))
                    queryCacheSizes_intel(l1,l2,l3,max_std_funcs);
                  else if(cpuid_is_vendor(abcd,"AuthenticAMD") || cpuid_is_vendor(abcd,"AMDisbetter!"))
                    queryCacheSizes_amd(l1,l2,l3);
                  else
                    // by default let's use Intel's API
                    queryCacheSizes_intel(l1,l2,l3,max_std_funcs);
                
                  // here is the list of other vendors:
                //   ||cpuid_is_vendor(abcd,"VIA VIA VIA ")
                //   ||cpuid_is_vendor(abcd,"CyrixInstead")
                //   ||cpuid_is_vendor(abcd,"CentaurHauls")
                //   ||cpuid_is_vendor(abcd,"GenuineTMx86")
                //   ||cpuid_is_vendor(abcd,"TransmetaCPU")
                //   ||cpuid_is_vendor(abcd,"RiseRiseRise")
                //   ||cpuid_is_vendor(abcd,"Geode by NSC")
                //   ||cpuid_is_vendor(abcd,"SiS SiS SiS ")
                //   ||cpuid_is_vendor(abcd,"UMC UMC UMC ")
                //   ||cpuid_is_vendor(abcd,"NexGenDriven")
                  #else
                  l1 = l2 = l3 = -1;
                  #endif
                }
                
                /** \internal
                 * \returns the size in Bytes of the L1 data cache */
                inline int queryL1CacheSize()
                {
                  int l1(-1), l2, l3;
                  queryCacheSizes(l1,l2,l3);
                  return l1;
                }
                
                /** \internal
                 * \returns the size in Bytes of the L2 or L3 cache if this later is present */
                inline int queryTopLevelCacheSize()
                {
                  int l1, l2(-1), l3(-1);
                  queryCacheSizes(l1,l2,l3);
                  return (std::max)(l2,l3);
                }
                
                } // end namespace internal
                
                #endif // EIGEN_MEMORY_H


Top 10 Lines:

     Line      Count

      492     246802

Execution Summary:

        1   Executable lines in this file
        1   Lines executed
   100.00   Percent of the file executed

   246802   Total number of line executions
246802.00   Average executions per line


*** File /cygdrive/c/Users/Paul/Documents/My Dropbox/Online Resources/My Homework/UBC 2011W/NSERC/matrix-factor/./Eigen/src/Core/Matrix.h:
                // This file is part of Eigen, a lightweight C++ template library
                // for linear algebra.
                //
                // Copyright (C) 2006-2010 Benoit Jacob <jacob.benoit.1@gmail.com>
                // Copyright (C) 2008-2009 Gael Guennebaud <gael.guennebaud@inria.fr>
                //
                // Eigen is free software; you can redistribute it and/or
                // modify it under the terms of the GNU Lesser General Public
                // License as published by the Free Software Foundation; either
                // version 3 of the License, or (at your option) any later version.
                //
                // Alternatively, you can redistribute it and/or
                // modify it under the terms of the GNU General Public License as
                // published by the Free Software Foundation; either version 2 of
                // the License, or (at your option) any later version.
                //
                // Eigen is distributed in the hope that it will be useful, but WITHOUT ANY
                // WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
                // FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License or the
                // GNU General Public License for more details.
                //
                // You should have received a copy of the GNU Lesser General Public
                // License and a copy of the GNU General Public License along with
                // Eigen. If not, see <http://www.gnu.org/licenses/>.
                
                #ifndef EIGEN_MATRIX_H
                #define EIGEN_MATRIX_H
                
                /** \class Matrix
                  * \ingroup Core_Module
                  *
                  * \brief The matrix class, also used for vectors and row-vectors
                  *
                  * The %Matrix class is the work-horse for all \em dense (\ref dense "note") matrices and vectors within Eigen.
                  * Vectors are matrices with one column, and row-vectors are matrices with one row.
                  *
                  * The %Matrix class encompasses \em both fixed-size and dynamic-size objects (\ref fixedsize "note").
                  *
                  * The first three template parameters are required:
                  * \tparam _Scalar \anchor matrix_tparam_scalar Numeric type, e.g. float, double, int or std::complex<float>.
                  *                 User defined sclar types are supported as well (see \ref user_defined_scalars "here").
                  * \tparam _Rows Number of rows, or \b Dynamic
                  * \tparam _Cols Number of columns, or \b Dynamic
                  *
                  * The remaining template parameters are optional -- in most cases you don't have to worry about them.
                  * \tparam _Options \anchor matrix_tparam_options A combination of either \b #RowMajor or \b #ColMajor, and of either
                  *                 \b #AutoAlign or \b #DontAlign.
                  *                 The former controls \ref TopicStorageOrders "storage order", and defaults to column-major. The latter controls alignment, which is required
                  *                 for vectorization. It defaults to aligning matrices except for fixed sizes that aren't a multiple of the packet size.
                  * \tparam _MaxRows Maximum number of rows. Defaults to \a _Rows (\ref maxrows "note").
                  * \tparam _MaxCols Maximum number of columns. Defaults to \a _Cols (\ref maxrows "note").
                  *
                  * Eigen provides a number of typedefs covering the usual cases. Here are some examples:
                  *
                  * \li \c Matrix2d is a 2x2 square matrix of doubles (\c Matrix<double, 2, 2>)
                  * \li \c Vector4f is a vector of 4 floats (\c Matrix<float, 4, 1>)
                  * \li \c RowVector3i is a row-vector of 3 ints (\c Matrix<int, 1, 3>)
                  *
                  * \li \c MatrixXf is a dynamic-size matrix of floats (\c Matrix<float, Dynamic, Dynamic>)
                  * \li \c VectorXf is a dynamic-size vector of floats (\c Matrix<float, Dynamic, 1>)
                  *
                  * \li \c Matrix2Xf is a partially fixed-size (dynamic-size) matrix of floats (\c Matrix<float, 2, Dynamic>)
                  * \li \c MatrixX3d is a partially dynamic-size (fixed-size) matrix of double (\c Matrix<double, Dynamic, 3>)
                  *
                  * See \link matrixtypedefs this page \endlink for a complete list of predefined \em %Matrix and \em Vector typedefs.
                  *
                  * You can access elements of vectors and matrices using normal subscripting:
                  *
                  * \code
                  * Eigen::VectorXd v(10);
                  * v[0] = 0.1;
                  * v[1] = 0.2;
                  * v(0) = 0.3;
                  * v(1) = 0.4;
                  *
                  * Eigen::MatrixXi m(10, 10);
                  * m(0, 1) = 1;
                  * m(0, 2) = 2;
                  * m(0, 3) = 3;
                  * \endcode
                  *
                  * This class can be extended with the help of the plugin mechanism described on the page
                  * \ref TopicCustomizingEigen by defining the preprocessor symbol \c EIGEN_MATRIX_PLUGIN.
                  *
                  * <i><b>Some notes:</b></i>
                  *
                  * <dl>
                  * <dt><b>\anchor dense Dense versus sparse:</b></dt>
                  * <dd>This %Matrix class handles dense, not sparse matrices and vectors. For sparse matrices and vectors, see the Sparse module.
                  *
                  * Dense matrices and vectors are plain usual arrays of coefficients. All the coefficients are stored, in an ordinary contiguous array.
                  * This is unlike Sparse matrices and vectors where the coefficients are stored as a list of nonzero coefficients.</dd>
                  *
                  * <dt><b>\anchor fixedsize Fixed-size versus dynamic-size:</b></dt>
                  * <dd>Fixed-size means that the numbers of rows and columns are known are compile-time. In this case, Eigen allocates the array
                  * of coefficients as a fixed-size array, as a class member. This makes sense for very small matrices, typically up to 4x4, sometimes up
                  * to 16x16. Larger matrices should be declared as dynamic-size even if one happens to know their size at compile-time.
                  *
                  * Dynamic-size means that the numbers of rows or columns are not necessarily known at compile-time. In this case they are runtime
                  * variables, and the array of coefficients is allocated dynamically on the heap.
                  *
                  * Note that \em dense matrices, be they Fixed-size or Dynamic-size, <em>do not</em> expand dynamically in the sense of a std::map.
                  * If you want this behavior, see the Sparse module.</dd>
                  *
                  * <dt><b>\anchor maxrows _MaxRows and _MaxCols:</b></dt>
                  * <dd>In most cases, one just leaves these parameters to the default values.
                  * These parameters mean the maximum size of rows and columns that the matrix may have. They are useful in cases
                  * when the exact numbers of rows and columns are not known are compile-time, but it is known at compile-time that they cannot
                  * exceed a certain value. This happens when taking dynamic-size blocks inside fixed-size matrices: in this case _MaxRows and _MaxCols
                  * are the dimensions of the original matrix, while _Rows and _Cols are Dynamic.</dd>
                  * </dl>
                  *
                  * \see MatrixBase for the majority of the API methods for matrices, \ref TopicClassHierarchy, 
                  * \ref TopicStorageOrders 
                  */
                
                namespace internal {
                template<typename _Scalar, int _Rows, int _Cols, int _Options, int _MaxRows, int _MaxCols>
                struct traits<Matrix<_Scalar, _Rows, _Cols, _Options, _MaxRows, _MaxCols> >
                {
                  typedef _Scalar Scalar;
                  typedef Dense StorageKind;
                  typedef DenseIndex Index;
                  typedef MatrixXpr XprKind;
                  enum {
                    RowsAtCompileTime = _Rows,
                    ColsAtCompileTime = _Cols,
                    MaxRowsAtCompileTime = _MaxRows,
                    MaxColsAtCompileTime = _MaxCols,
                    Flags = compute_matrix_flags<_Scalar, _Rows, _Cols, _Options, _MaxRows, _MaxCols>::ret,
                    CoeffReadCost = NumTraits<Scalar>::ReadCost,
                    Options = _Options,
                    InnerStrideAtCompileTime = 1,
                    OuterStrideAtCompileTime = (Options&RowMajor) ? ColsAtCompileTime : RowsAtCompileTime
                  };
                };
                }
                
                template<typename _Scalar, int _Rows, int _Cols, int _Options, int _MaxRows, int _MaxCols>
                class Matrix
                  : public PlainObjectBase<Matrix<_Scalar, _Rows, _Cols, _Options, _MaxRows, _MaxCols> >
                {
                  public:
                
                    /** \brief Base class typedef.
                      * \sa PlainObjectBase
                      */
                    typedef PlainObjectBase<Matrix> Base;
                
                    enum { Options = _Options };
                
                    EIGEN_DENSE_PUBLIC_INTERFACE(Matrix)
                
                    typedef typename Base::PlainObject PlainObject;
                
                    using Base::base;
                    using Base::coeffRef;
                
                    /**
                      * \brief Assigns matrices to each other.
                      *
                      * \note This is a special case of the templated operator=. Its purpose is
                      * to prevent a default operator= from hiding the templated operator=.
                      *
                      * \callgraph
                      */
                    EIGEN_STRONG_INLINE Matrix& operator=(const Matrix& other)
                    {
                      return Base::_set(other);
                    }
                
                    /** \internal
                      * \brief Copies the value of the expression \a other into \c *this with automatic resizing.
                      *
                      * *this might be resized to match the dimensions of \a other. If *this was a null matrix (not already initialized),
                      * it will be initialized.
                      *
                      * Note that copying a row-vector into a vector (and conversely) is allowed.
                      * The resizing, if any, is then done in the appropriate way so that row-vectors
                      * remain row-vectors and vectors remain vectors.
                      */
                    template<typename OtherDerived>
                    EIGEN_STRONG_INLINE Matrix& operator=(const MatrixBase<OtherDerived>& other)
                    {
                      return Base::_set(other);
                    }
                
                    /* Here, doxygen failed to copy the brief information when using \copydoc */
                
                    /**
                      * \brief Copies the generic expression \a other into *this.
                      * \copydetails DenseBase::operator=(const EigenBase<OtherDerived> &other)
                      */
                    template<typename OtherDerived>
                    EIGEN_STRONG_INLINE Matrix& operator=(const EigenBase<OtherDerived> &other)
                    {
                      return Base::operator=(other);
                    }
                
                    template<typename OtherDerived>
                    EIGEN_STRONG_INLINE Matrix& operator=(const ReturnByValue<OtherDerived>& func)
                    {
                      return Base::operator=(func);
                    }
                
                    /** \brief Default constructor.
                      *
                      * For fixed-size matrices, does nothing.
                      *
                      * For dynamic-size matrices, creates an empty matrix of size 0. Does not allocate any array. Such a matrix
                      * is called a null matrix. This constructor is the unique way to create null matrices: resizing
                      * a matrix to 0 is not supported.
                      *
                      * \sa resize(Index,Index)
                      */
                    EIGEN_STRONG_INLINE explicit Matrix() : Base()
                    {
                      Base::_check_template_params();
                      EIGEN_INITIALIZE_BY_ZERO_IF_THAT_OPTION_IS_ENABLED
                    }
                
                    // FIXME is it still needed
                    Matrix(internal::constructor_without_unaligned_array_assert)
                      : Base(internal::constructor_without_unaligned_array_assert())
                    { Base::_check_template_params(); EIGEN_INITIALIZE_BY_ZERO_IF_THAT_OPTION_IS_ENABLED }
                
                    /** \brief Constructs a vector or row-vector with given dimension. \only_for_vectors
                      *
                      * Note that this is only useful for dynamic-size vectors. For fixed-size vectors,
                      * it is redundant to pass the dimension here, so it makes more sense to use the default
                      * constructor Matrix() instead.
                      */
                    EIGEN_STRONG_INLINE explicit Matrix(Index dim)
           1 ->       : Base(dim, RowsAtCompileTime == 1 ? 1 : dim, ColsAtCompileTime == 1 ? 1 : dim)
                    {
                      Base::_check_template_params();
                      EIGEN_STATIC_ASSERT_VECTOR_ONLY(Matrix)
                      eigen_assert(dim >= 0);
                      eigen_assert(SizeAtCompileTime == Dynamic || SizeAtCompileTime == dim);
                      EIGEN_INITIALIZE_BY_ZERO_IF_THAT_OPTION_IS_ENABLED
                    }
                
                    #ifndef EIGEN_PARSED_BY_DOXYGEN
                    template<typename T0, typename T1>
                    EIGEN_STRONG_INLINE Matrix(const T0& x, const T1& y)
                    {
                      Base::_check_template_params();
                      Base::template _init2<T0,T1>(x, y);
                    }
                    #else
                    /** \brief Constructs an uninitialized matrix with \a rows rows and \a cols columns.
                      *
                      * This is useful for dynamic-size matrices. For fixed-size matrices,
                      * it is redundant to pass these parameters, so one should use the default constructor
                      * Matrix() instead. */
                    Matrix(Index rows, Index cols);
                    /** \brief Constructs an initialized 2D vector with given coefficients */
                    Matrix(const Scalar& x, const Scalar& y);
                    #endif
                
                    /** \brief Constructs an initialized 3D vector with given coefficients */
                    EIGEN_STRONG_INLINE Matrix(const Scalar& x, const Scalar& y, const Scalar& z)
                    {
                      Base::_check_template_params();
                      EIGEN_STATIC_ASSERT_VECTOR_SPECIFIC_SIZE(Matrix, 3)
                      m_storage.data()[0] = x;
                      m_storage.data()[1] = y;
                      m_storage.data()[2] = z;
                    }
                    /** \brief Constructs an initialized 4D vector with given coefficients */
                    EIGEN_STRONG_INLINE Matrix(const Scalar& x, const Scalar& y, const Scalar& z, const Scalar& w)
                    {
                      Base::_check_template_params();
                      EIGEN_STATIC_ASSERT_VECTOR_SPECIFIC_SIZE(Matrix, 4)
                      m_storage.data()[0] = x;
                      m_storage.data()[1] = y;
                      m_storage.data()[2] = z;
                      m_storage.data()[3] = w;
                    }
                
                    explicit Matrix(const Scalar *data);
                
                    /** \brief Constructor copying the value of the expression \a other */
                    template<typename OtherDerived>
                    EIGEN_STRONG_INLINE Matrix(const MatrixBase<OtherDerived>& other)
                             : Base(other.rows() * other.cols(), other.rows(), other.cols())
                    {
                      // This test resides here, to bring the error messages closer to the user. Normally, these checks
                      // are performed deeply within the library, thus causing long and scary error traces.
                      EIGEN_STATIC_ASSERT((internal::is_same<Scalar, typename OtherDerived::Scalar>::value),
                        YOU_MIXED_DIFFERENT_NUMERIC_TYPES__YOU_NEED_TO_USE_THE_CAST_METHOD_OF_MATRIXBASE_TO_CAST_NUMERIC_TYPES_EXPLICITLY)
                
                      Base::_check_template_params();
                      Base::_set_noalias(other);
                    }
                    /** \brief Copy constructor */
                    EIGEN_STRONG_INLINE Matrix(const Matrix& other)
                            : Base(other.rows() * other.cols(), other.rows(), other.cols())
                    {
                      Base::_check_template_params();
                      Base::_set_noalias(other);
                    }
                    /** \brief Copy constructor with in-place evaluation */
                    template<typename OtherDerived>
                    EIGEN_STRONG_INLINE Matrix(const ReturnByValue<OtherDerived>& other)
                    {
                      Base::_check_template_params();
                      Base::resize(other.rows(), other.cols());
                      other.evalTo(*this);
                    }
                
                    /** \brief Copy constructor for generic expressions.
                      * \sa MatrixBase::operator=(const EigenBase<OtherDerived>&)
                      */
                    template<typename OtherDerived>
                    EIGEN_STRONG_INLINE Matrix(const EigenBase<OtherDerived> &other)
                      : Base(other.derived().rows() * other.derived().cols(), other.derived().rows(), other.derived().cols())
                    {
                      Base::_check_template_params();
                      Base::resize(other.rows(), other.cols());
                      // FIXME/CHECK: isn't *this = other.derived() more efficient. it allows to
                      //              go for pure _set() implementations, right?
                      *this = other;
                    }
                
                    /** \internal
                      * \brief Override MatrixBase::swap() since for dynamic-sized matrices
                      * of same type it is enough to swap the data pointers.
                      */
                    template<typename OtherDerived>
                    void swap(MatrixBase<OtherDerived> const & other)
                    { this->_swap(other.derived()); }
                
                    inline Index innerStride() const { return 1; }
                    inline Index outerStride() const { return this->innerSize(); }
                
                    /////////// Geometry module ///////////
                
                    template<typename OtherDerived>
                    explicit Matrix(const RotationBase<OtherDerived,ColsAtCompileTime>& r);
                    template<typename OtherDerived>
                    Matrix& operator=(const RotationBase<OtherDerived,ColsAtCompileTime>& r);
                
                    #ifdef EIGEN2_SUPPORT
                    template<typename OtherDerived>
                    explicit Matrix(const eigen2_RotationBase<OtherDerived,ColsAtCompileTime>& r);
                    template<typename OtherDerived>
                    Matrix& operator=(const eigen2_RotationBase<OtherDerived,ColsAtCompileTime>& r);
                    #endif
                
                    // allow to extend Matrix outside Eigen
                    #ifdef EIGEN_MATRIX_PLUGIN
                    #include EIGEN_MATRIX_PLUGIN
                    #endif
                
                  protected:
                    template <typename Derived, typename OtherDerived, bool IsVector>
                    friend struct internal::conservative_resize_like_impl;
                
                    using Base::m_storage;
                };
                
                /** \defgroup matrixtypedefs Global matrix typedefs
                  *
                  * \ingroup Core_Module
                  *
                  * Eigen defines several typedef shortcuts for most common matrix and vector types.
                  *
                  * The general patterns are the following:
                  *
                  * \c MatrixSizeType where \c Size can be \c 2,\c 3,\c 4 for fixed size square matrices or \c X for dynamic size,
                  * and where \c Type can be \c i for integer, \c f for float, \c d for double, \c cf for complex float, \c cd
                  * for complex double.
                  *
                  * For example, \c Matrix3d is a fixed-size 3x3 matrix type of doubles, and \c MatrixXf is a dynamic-size matrix of floats.
                  *
                  * There are also \c VectorSizeType and \c RowVectorSizeType which are self-explanatory. For example, \c Vector4cf is
                  * a fixed-size vector of 4 complex floats.
                  *
                  * \sa class Matrix
                  */
                
                #define EIGEN_MAKE_TYPEDEFS(Type, TypeSuffix, Size, SizeSuffix)   \
                /** \ingroup matrixtypedefs */                                    \
                typedef Matrix<Type, Size, Size> Matrix##SizeSuffix##TypeSuffix;  \
                /** \ingroup matrixtypedefs */                                    \
                typedef Matrix<Type, Size, 1>    Vector##SizeSuffix##TypeSuffix;  \
                /** \ingroup matrixtypedefs */                                    \
                typedef Matrix<Type, 1, Size>    RowVector##SizeSuffix##TypeSuffix;
                
                #define EIGEN_MAKE_FIXED_TYPEDEFS(Type, TypeSuffix, Size)         \
                /** \ingroup matrixtypedefs */                                    \
                typedef Matrix<Type, Size, Dynamic> Matrix##Size##X##TypeSuffix;  \
                /** \ingroup matrixtypedefs */                                    \
                typedef Matrix<Type, Dynamic, Size> Matrix##X##Size##TypeSuffix;
                
                #define EIGEN_MAKE_TYPEDEFS_ALL_SIZES(Type, TypeSuffix) \
                EIGEN_MAKE_TYPEDEFS(Type, TypeSuffix, 2, 2) \
                EIGEN_MAKE_TYPEDEFS(Type, TypeSuffix, 3, 3) \
                EIGEN_MAKE_TYPEDEFS(Type, TypeSuffix, 4, 4) \
                EIGEN_MAKE_TYPEDEFS(Type, TypeSuffix, Dynamic, X) \
                EIGEN_MAKE_FIXED_TYPEDEFS(Type, TypeSuffix, 2) \
                EIGEN_MAKE_FIXED_TYPEDEFS(Type, TypeSuffix, 3) \
                EIGEN_MAKE_FIXED_TYPEDEFS(Type, TypeSuffix, 4)
                
                EIGEN_MAKE_TYPEDEFS_ALL_SIZES(int,                  i)
                EIGEN_MAKE_TYPEDEFS_ALL_SIZES(float,                f)
                EIGEN_MAKE_TYPEDEFS_ALL_SIZES(double,               d)
                EIGEN_MAKE_TYPEDEFS_ALL_SIZES(std::complex<float>,  cf)
                EIGEN_MAKE_TYPEDEFS_ALL_SIZES(std::complex<double>, cd)
                
                #undef EIGEN_MAKE_TYPEDEFS_ALL_SIZES
                #undef EIGEN_MAKE_TYPEDEFS
                #undef EIGEN_MAKE_FIXED_TYPEDEFS
                
                #endif // EIGEN_MATRIX_H


Top 10 Lines:

     Line      Count

      234          1

Execution Summary:

        1   Executable lines in this file
        1   Lines executed
   100.00   Percent of the file executed

        1   Total number of line executions
     1.00   Average executions per line


*** File /usr/lib/gcc/i686-pc-cygwin/3.4.4/include/c++/bits/stl_algobase.h:
                // Bits and pieces used in algorithms -*- C++ -*-
                
                // Copyright (C) 2001, 2002, 2003, 2004, 2005 Free Software Foundation, Inc.
                //
                // This file is part of the GNU ISO C++ Library.  This library is free
                // software; you can redistribute it and/or modify it under the
                // terms of the GNU General Public License as published by the
                // Free Software Foundation; either version 2, or (at your option)
                // any later version.
                
                // This library is distributed in the hope that it will be useful,
                // but WITHOUT ANY WARRANTY; without even the implied warranty of
                // MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
                // GNU General Public License for more details.
                
                // You should have received a copy of the GNU General Public License along
                // with this library; see the file COPYING.  If not, write to the Free
                // Software Foundation, 59 Temple Place - Suite 330, Boston, MA 02111-1307,
                // USA.
                
                // As a special exception, you may use this file as part of a free software
                // library without restriction.  Specifically, if other files instantiate
                // templates or use macros or inline functions from this file, or you compile
                // this file and link it with other files to produce an executable, this
                // file does not by itself cause the resulting executable to be covered by
                // the GNU General Public License.  This exception does not however
                // invalidate any other reasons why the executable file might be covered by
                // the GNU General Public License.
                
                /*
                 *
                 * Copyright (c) 1994
                 * Hewlett-Packard Company
                 *
                 * Permission to use, copy, modify, distribute and sell this software
                 * and its documentation for any purpose is hereby granted without fee,
                 * provided that the above copyright notice appear in all copies and
                 * that both that copyright notice and this permission notice appear
                 * in supporting documentation.  Hewlett-Packard Company makes no
                 * representations about the suitability of this software for any
                 * purpose.  It is provided "as is" without express or implied warranty.
                 *
                 *
                 * Copyright (c) 1996-1998
                 * Silicon Graphics Computer Systems, Inc.
                 *
                 * Permission to use, copy, modify, distribute and sell this software
                 * and its documentation for any purpose is hereby granted without fee,
                 * provided that the above copyright notice appear in all copies and
                 * that both that copyright notice and this permission notice appear
                 * in supporting documentation.  Silicon Graphics makes no
                 * representations about the suitability of this software for any
                 * purpose.  It is provided "as is" without express or implied warranty.
                 */
                
                /** @file stl_algobase.h
                 *  This is an internal header file, included by other library headers.
                 *  You should not attempt to use it directly.
                 */
                
                #ifndef _ALGOBASE_H
                #define _ALGOBASE_H 1
                
                #include <bits/c++config.h>
                #include <cstring>
                #include <climits>
                #include <cstdlib>
                #include <cstddef>
                #include <new>
                #include <iosfwd>
                #include <bits/stl_pair.h>
                #include <bits/type_traits.h>
                #include <bits/stl_iterator_base_types.h>
                #include <bits/stl_iterator_base_funcs.h>
                #include <bits/stl_iterator.h>
                #include <bits/concept_check.h>
                #include <debug/debug.h>
                
                namespace std
                {
                  /**
                   *  @brief Swaps the contents of two iterators.
                   *  @param  a  An iterator.
                   *  @param  b  Another iterator.
                   *  @return   Nothing.
                   *
                   *  This function swaps the values pointed to by two iterators, not the
                   *  iterators themselves.
                  */
                  template<typename _ForwardIterator1, typename _ForwardIterator2>
                    inline void
                    iter_swap(_ForwardIterator1 __a, _ForwardIterator2 __b)
                    {
                      typedef typename iterator_traits<_ForwardIterator1>::value_type
                	_ValueType1;
                      typedef typename iterator_traits<_ForwardIterator2>::value_type
                	_ValueType2;
                
                      // concept requirements
                      __glibcxx_function_requires(_Mutable_ForwardIteratorConcept<
                				  _ForwardIterator1>)
                      __glibcxx_function_requires(_Mutable_ForwardIteratorConcept<
                				  _ForwardIterator2>)
                      __glibcxx_function_requires(_ConvertibleConcept<_ValueType1,
                				  _ValueType2>)
                      __glibcxx_function_requires(_ConvertibleConcept<_ValueType2,
                				  _ValueType1>)
                
                      const _ValueType1 __tmp = *__a;
                      *__a = *__b;
                      *__b = __tmp;
                    }
                
                  /**
                   *  @brief Swaps two values.
                   *  @param  a  A thing of arbitrary type.
                   *  @param  b  Another thing of arbitrary type.
                   *  @return   Nothing.
                   *
                   *  This is the simple classic generic implementation.  It will work on
                   *  any type which has a copy constructor and an assignment operator.
                  */
                  template<typename _Tp>
                    inline void
                    swap(_Tp& __a, _Tp& __b)
                    {
                      // concept requirements
                      __glibcxx_function_requires(_SGIAssignableConcept<_Tp>)
                
                      const _Tp __tmp = __a;
                      __a = __b;
                      __b = __tmp;
                    }
                
                  #undef min
                  #undef max
                
                  /**
                   *  @brief This does what you think it does.
                   *  @param  a  A thing of arbitrary type.
                   *  @param  b  Another thing of arbitrary type.
                   *  @return   The lesser of the parameters.
                   *
                   *  This is the simple classic generic implementation.  It will work on
                   *  temporary expressions, since they are only evaluated once, unlike a
                   *  preprocessor macro.
                  */
                  template<typename _Tp>
                    inline const _Tp&
                    min(const _Tp& __a, const _Tp& __b)
                    {
                      // concept requirements
                      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
                      //return __b < __a ? __b : __a;
                      if (__b < __a)
                	return __b;
                      return __a;
                    }
                
                  /**
                   *  @brief This does what you think it does.
                   *  @param  a  A thing of arbitrary type.
                   *  @param  b  Another thing of arbitrary type.
                   *  @return   The greater of the parameters.
                   *
                   *  This is the simple classic generic implementation.  It will work on
                   *  temporary expressions, since they are only evaluated once, unlike a
                   *  preprocessor macro.
                  */
                  template<typename _Tp>
                    inline const _Tp&
                    max(const _Tp& __a, const _Tp& __b)
                    {
                      // concept requirements
                      __glibcxx_function_requires(_LessThanComparableConcept<_Tp>)
                      //return  __a < __b ? __b : __a;
                      if (__a < __b)
                	return __b;
                      return __a;
                    }
                
                  /**
                   *  @brief This does what you think it does.
                   *  @param  a  A thing of arbitrary type.
                   *  @param  b  Another thing of arbitrary type.
                   *  @param  comp  A @link s20_3_3_comparisons comparison functor@endlink.
                   *  @return   The lesser of the parameters.
                   *
                   *  This will work on temporary expressions, since they are only evaluated
                   *  once, unlike a preprocessor macro.
                  */
                  template<typename _Tp, typename _Compare>
                    inline const _Tp&
                    min(const _Tp& __a, const _Tp& __b, _Compare __comp)
                    {
                      //return __comp(__b, __a) ? __b : __a;
                      if (__comp(__b, __a))
                	return __b;
                      return __a;
                    }
                
                  /**
                   *  @brief This does what you think it does.
                   *  @param  a  A thing of arbitrary type.
                   *  @param  b  Another thing of arbitrary type.
                   *  @param  comp  A @link s20_3_3_comparisons comparison functor@endlink.
                   *  @return   The greater of the parameters.
                   *
                   *  This will work on temporary expressions, since they are only evaluated
                   *  once, unlike a preprocessor macro.
                  */
                  template<typename _Tp, typename _Compare>
                    inline const _Tp&
                    max(const _Tp& __a, const _Tp& __b, _Compare __comp)
                    {
                      //return __comp(__a, __b) ? __b : __a;
                      if (__comp(__a, __b))
                	return __b;
                      return __a;
                    }
                
                  // All of these auxiliary functions serve two purposes.  (1) Replace
                  // calls to copy with memmove whenever possible.  (Memmove, not memcpy,
                  // because the input and output ranges are permitted to overlap.)
                  // (2) If we're using random access iterators, then write the loop as
                  // a for loop with an explicit count.
                
                  template<typename _InputIterator, typename _OutputIterator>
                    inline _OutputIterator
                    __copy(_InputIterator __first, _InputIterator __last,
                	   _OutputIterator __result, input_iterator_tag)
                    {
                      for (; __first != __last; ++__result, ++__first)
                	*__result = *__first;
                      return __result;
                    }
                
                  template<typename _RandomAccessIterator, typename _OutputIterator>
                    inline _OutputIterator
                    __copy(_RandomAccessIterator __first, _RandomAccessIterator __last,
                	   _OutputIterator __result, random_access_iterator_tag)
                    {
                      typedef typename iterator_traits<_RandomAccessIterator>::difference_type
                          _Distance;
                      for (_Distance __n = __last - __first; __n > 0; --__n)
                	{
                	  *__result = *__first;
                	  ++__first;
                	  ++__result;
                	}
                      return __result;
                    }
                
                  template<typename _Tp>
                    inline _Tp*
                    __copy_trivial(const _Tp* __first, const _Tp* __last, _Tp* __result)
                    {
                      std::memmove(__result, __first, sizeof(_Tp) * (__last - __first));
                      return __result + (__last - __first);
                    }
                
                  template<typename _InputIterator, typename _OutputIterator>
                    inline _OutputIterator
                    __copy_aux2(_InputIterator __first, _InputIterator __last,
                		_OutputIterator __result, __false_type)
                    { return std::__copy(__first, __last, __result,
                			 std::__iterator_category(__first)); }
                
                  template<typename _InputIterator, typename _OutputIterator>
                    inline _OutputIterator
                    __copy_aux2(_InputIterator __first, _InputIterator __last,
                		_OutputIterator __result, __true_type)
                    { return std::__copy(__first, __last, __result,
                			 std::__iterator_category(__first)); }
                
                  template<typename _Tp>
                    inline _Tp*
                    __copy_aux2(_Tp* __first, _Tp* __last, _Tp* __result, __true_type)
                    { return std::__copy_trivial(__first, __last, __result); }
                
                  template<typename _Tp>
                    inline _Tp*
                    __copy_aux2(const _Tp* __first, const _Tp* __last, _Tp* __result,
                		__true_type)
                    { return std::__copy_trivial(__first, __last, __result); }
                
                  template<typename _InputIterator, typename _OutputIterator>
                    inline _OutputIterator
                    __copy_ni2(_InputIterator __first, _InputIterator __last,
                	       _OutputIterator __result, __true_type)
                    {
                      typedef typename iterator_traits<_InputIterator>::value_type
                	_ValueType;
                      typedef typename __type_traits<
                	_ValueType>::has_trivial_assignment_operator _Trivial;
                      return _OutputIterator(std::__copy_aux2(__first, __last, __result.base(),
                					      _Trivial()));
                    }
                
                  template<typename _InputIterator, typename _OutputIterator>
                    inline _OutputIterator
                    __copy_ni2(_InputIterator __first, _InputIterator __last,
                	       _OutputIterator __result, __false_type)
                    {
                      typedef typename iterator_traits<_InputIterator>::value_type _ValueType;
                      typedef typename __type_traits<
                	_ValueType>::has_trivial_assignment_operator _Trivial;
                      return std::__copy_aux2(__first, __last, __result, _Trivial());
                    }
                
                  template<typename _InputIterator, typename _OutputIterator>
                    inline _OutputIterator
                    __copy_ni1(_InputIterator __first, _InputIterator __last,
                	       _OutputIterator __result, __true_type)
                    {
                      typedef typename _Is_normal_iterator<_OutputIterator>::_Normal __Normal;
                      return std::__copy_ni2(__first.base(), __last.base(),
                			     __result, __Normal());
                    }
                
                  template<typename _InputIterator, typename _OutputIterator>
                    inline _OutputIterator
                    __copy_ni1(_InputIterator __first, _InputIterator __last,
                	       _OutputIterator __result, __false_type)
                    {
                      typedef typename _Is_normal_iterator<_OutputIterator>::_Normal __Normal;
                      return std::__copy_ni2(__first, __last, __result, __Normal());
                    }
                
                  /**
                   *  @brief Copies the range [first,last) into result.
                   *  @param  first  An input iterator.
                   *  @param  last   An input iterator.
                   *  @param  result An output iterator.
                   *  @return   result + (first - last)
                   *
                   *  This inline function will boil down to a call to @c memmove whenever
                   *  possible.  Failing that, if random access iterators are passed, then the
                   *  loop count will be known (and therefore a candidate for compiler
                   *  optimizations such as unrolling).  Result may not be contained within
                   *  [first,last); the copy_backward function should be used instead.
                   *
                   *  Note that the end of the output range is permitted to be contained
                   *  within [first,last).
                  */
                  template<typename _InputIterator, typename _OutputIterator>
                    inline _OutputIterator
                    copy(_InputIterator __first, _InputIterator __last,
                	 _OutputIterator __result)
                    {
                      // concept requirements
                      __glibcxx_function_requires(_InputIteratorConcept<_InputIterator>)
                      __glibcxx_function_requires(_OutputIteratorConcept<_OutputIterator,
                	    typename iterator_traits<_InputIterator>::value_type>)
                      __glibcxx_requires_valid_range(__first, __last);
                
                       typedef typename _Is_normal_iterator<_InputIterator>::_Normal __Normal;
                       return std::__copy_ni1(__first, __last, __result, __Normal());
                    }
                
                  template<typename _BidirectionalIterator1, typename _BidirectionalIterator2>
                    inline _BidirectionalIterator2
                    __copy_backward(_BidirectionalIterator1 __first,
                		    _BidirectionalIterator1 __last,
                		    _BidirectionalIterator2 __result,
                		    bidirectional_iterator_tag)
                    {
                      while (__first != __last)
                        *--__result = *--__last;
                      return __result;
                    }
                
                  template<typename _RandomAccessIterator, typename _BidirectionalIterator>
                    inline _BidirectionalIterator
                    __copy_backward(_RandomAccessIterator __first, _RandomAccessIterator __last,
                		    _BidirectionalIterator __result, random_access_iterator_tag)
                    {
                      typename iterator_traits<_RandomAccessIterator>::difference_type __n;
                      for (__n = __last - __first; __n > 0; --__n)
                        *--__result = *--__last;
                      return __result;
                    }
                
                
                  // This dispatch class is a workaround for compilers that do not
                  // have partial ordering of function templates.  All we're doing is
                  // creating a specialization so that we can turn a call to copy_backward
                  // into a memmove whenever possible.
                  template<typename _BidirectionalIterator1, typename _BidirectionalIterator2,
                           typename _BoolType>
                    struct __copy_backward_dispatch
                    {
                      static _BidirectionalIterator2
                      copy(_BidirectionalIterator1 __first, _BidirectionalIterator1 __last,
                	   _BidirectionalIterator2 __result)
                      { return std::__copy_backward(__first, __last, __result,
                				    std::__iterator_category(__first)); }
                    };
                
                  template<typename _Tp>
                    struct __copy_backward_dispatch<_Tp*, _Tp*, __true_type>
                    {
                      static _Tp*
                      copy(const _Tp* __first, const _Tp* __last, _Tp* __result)
                      {
                	const ptrdiff_t _Num = __last - __first;
                	std::memmove(__result - _Num, __first, sizeof(_Tp) * _Num);
                	return __result - _Num;
                      }
                    };
                
                  template<typename _Tp>
                    struct __copy_backward_dispatch<const _Tp*, _Tp*, __true_type>
                    {
                      static _Tp*
                      copy(const _Tp* __first, const _Tp* __last, _Tp* __result)
                      {
                	return  std::__copy_backward_dispatch<_Tp*, _Tp*, __true_type>
                	  ::copy(__first, __last, __result);
                      }
                    };
                
                  template<typename _BI1, typename _BI2>
                    inline _BI2
                    __copy_backward_aux(_BI1 __first, _BI1 __last, _BI2 __result)
                    {
                      typedef typename __type_traits<typename iterator_traits<_BI2>::value_type>
                			    ::has_trivial_assignment_operator _Trivial;
                      return
                	std::__copy_backward_dispatch<_BI1, _BI2, _Trivial>::copy(__first,
                								  __last,
                								  __result);
                    }
                
                  template <typename _BI1, typename _BI2>
                    inline _BI2
                    __copy_backward_output_normal_iterator(_BI1 __first, _BI1 __last,
                					   _BI2 __result, __true_type)
                    { return _BI2(std::__copy_backward_aux(__first, __last, __result.base())); }
                
                  template <typename _BI1, typename _BI2>
                    inline _BI2
                    __copy_backward_output_normal_iterator(_BI1 __first, _BI1 __last,
                					   _BI2 __result, __false_type)
                    { return std::__copy_backward_aux(__first, __last, __result); }
                
                  template <typename _BI1, typename _BI2>
                    inline _BI2
                    __copy_backward_input_normal_iterator(_BI1 __first, _BI1 __last,
                					  _BI2 __result, __true_type)
                    {
                      typedef typename _Is_normal_iterator<_BI2>::_Normal __Normal;
                      return std::__copy_backward_output_normal_iterator(__first.base(),
                							 __last.base(),
                							 __result, __Normal());
                    }
                
                  template <typename _BI1, typename _BI2>
                    inline _BI2
                    __copy_backward_input_normal_iterator(_BI1 __first, _BI1 __last,
                					  _BI2 __result, __false_type)
                    {
                      typedef typename _Is_normal_iterator<_BI2>::_Normal __Normal;
                      return std::__copy_backward_output_normal_iterator(__first, __last,
                							 __result, __Normal());
                    }
                
                  /**
                   *  @brief Copies the range [first,last) into result.
                   *  @param  first  A bidirectional iterator.
                   *  @param  last   A bidirectional iterator.
                   *  @param  result A bidirectional iterator.
                   *  @return   result - (first - last)
                   *
                   *  The function has the same effect as copy, but starts at the end of the
                   *  range and works its way to the start, returning the start of the result.
                   *  This inline function will boil down to a call to @c memmove whenever
                   *  possible.  Failing that, if random access iterators are passed, then the
                   *  loop count will be known (and therefore a candidate for compiler
                   *  optimizations such as unrolling).
                   *
                   *  Result may not be in the range [first,last).  Use copy instead.  Note
                   *  that the start of the output range may overlap [first,last).
                  */
                  template <typename _BI1, typename _BI2>
                    inline _BI2
                    copy_backward(_BI1 __first, _BI1 __last, _BI2 __result)
                    {
                      // concept requirements
                      __glibcxx_function_requires(_BidirectionalIteratorConcept<_BI1>)
                      __glibcxx_function_requires(_Mutable_BidirectionalIteratorConcept<_BI2>)
                      __glibcxx_function_requires(_ConvertibleConcept<
                	    typename iterator_traits<_BI1>::value_type,
                	    typename iterator_traits<_BI2>::value_type>)
                      __glibcxx_requires_valid_range(__first, __last);
                
                      typedef typename _Is_normal_iterator<_BI1>::_Normal __Normal;
                      return std::__copy_backward_input_normal_iterator(__first, __last,
                							__result, __Normal());
                    }
                
                
                  /**
                   *  @brief Fills the range [first,last) with copies of value.
                   *  @param  first  A forward iterator.
                   *  @param  last   A forward iterator.
                   *  @param  value  A reference-to-const of arbitrary type.
                   *  @return   Nothing.
                   *
                   *  This function fills a range with copies of the same value.  For one-byte
                   *  types filling contiguous areas of memory, this becomes an inline call to
                   *  @c memset.
                  */
                  template<typename _ForwardIterator, typename _Tp>
                    void
                    fill(_ForwardIterator __first, _ForwardIterator __last, const _Tp& __value)
           1 ->     {
                      // concept requirements
                      __glibcxx_function_requires(_Mutable_ForwardIteratorConcept<
                				  _ForwardIterator>)
                      __glibcxx_requires_valid_range(__first, __last);
                
       ##### ->       for ( ; __first != __last; ++__first)
                	*__first = __value;
                    }
                
                  /**
                   *  @brief Fills the range [first,first+n) with copies of value.
                   *  @param  first  An output iterator.
                   *  @param  n      The count of copies to perform.
                   *  @param  value  A reference-to-const of arbitrary type.
                   *  @return   The iterator at first+n.
                   *
                   *  This function fills a range with copies of the same value.  For one-byte
                   *  types filling contiguous areas of memory, this becomes an inline call to
                   *  @c memset.
                  */
                  template<typename _OutputIterator, typename _Size, typename _Tp>
                    _OutputIterator
                    fill_n(_OutputIterator __first, _Size __n, const _Tp& __value)
                    {
                      // concept requirements
                      __glibcxx_function_requires(_OutputIteratorConcept<_OutputIterator,_Tp>)
                
                      for ( ; __n > 0; --__n, ++__first)
                	*__first = __value;
                      return __first;
                    }
                
                  // Specialization: for one-byte types we can use memset.
                  inline void
                  fill(unsigned char* __first, unsigned char* __last, const unsigned char& __c)
                  {
                    __glibcxx_requires_valid_range(__first, __last);
                    const unsigned char __tmp = __c;
                    std::memset(__first, __tmp, __last - __first);
                  }
                
                  inline void
                  fill(signed char* __first, signed char* __last, const signed char& __c)
                  {
                    __glibcxx_requires_valid_range(__first, __last);
                    const signed char __tmp = __c;
                    std::memset(__first, static_cast<unsigned char>(__tmp), __last - __first);
                  }
                
                  inline void
                  fill(char* __first, char* __last, const char& __c)
                  {
                    __glibcxx_requires_valid_range(__first, __last);
                    const char __tmp = __c;
                    std::memset(__first, static_cast<unsigned char>(__tmp), __last - __first);
                  }
                
                  template<typename _Size>
                    inline unsigned char*
                    fill_n(unsigned char* __first, _Size __n, const unsigned char& __c)
                    {
                      std::fill(__first, __first + __n, __c);
                      return __first + __n;
                    }
                
                  template<typename _Size>
                    inline signed char*
                    fill_n(char* __first, _Size __n, const signed char& __c)
                    {
                      std::fill(__first, __first + __n, __c);
                      return __first + __n;
                    }
                
                  template<typename _Size>
                    inline char*
                    fill_n(char* __first, _Size __n, const char& __c)
                    {
                      std::fill(__first, __first + __n, __c);
                      return __first + __n;
                    }
                
                
                  /**
                   *  @brief Finds the places in ranges which don't match.
                   *  @param  first1  An input iterator.
                   *  @param  last1   An input iterator.
                   *  @param  first2  An input iterator.
                   *  @return   A pair of iterators pointing to the first mismatch.
                   *
                   *  This compares the elements of two ranges using @c == and returns a pair
                   *  of iterators.  The first iterator points into the first range, the
                   *  second iterator points into the second range, and the elements pointed
                   *  to by the iterators are not equal.
                  */
                  template<typename _InputIterator1, typename _InputIterator2>
                    pair<_InputIterator1, _InputIterator2>
                    mismatch(_InputIterator1 __first1, _InputIterator1 __last1,
                	     _InputIterator2 __first2)
                    {
                      // concept requirements
                      __glibcxx_function_requires(_InputIteratorConcept<_InputIterator1>)
                      __glibcxx_function_requires(_InputIteratorConcept<_InputIterator2>)
                      __glibcxx_function_requires(_EqualOpConcept<
                	    typename iterator_traits<_InputIterator1>::value_type,
                	    typename iterator_traits<_InputIterator2>::value_type>)
                      __glibcxx_requires_valid_range(__first1, __last1);
                
                      while (__first1 != __last1 && *__first1 == *__first2)
                        {
                	  ++__first1;
                	  ++__first2;
                        }
                      return pair<_InputIterator1, _InputIterator2>(__first1, __first2);
                    }
                
                  /**
                   *  @brief Finds the places in ranges which don't match.
                   *  @param  first1  An input iterator.
                   *  @param  last1   An input iterator.
                   *  @param  first2  An input iterator.
                   *  @param  binary_pred  A binary predicate @link s20_3_1_base functor@endlink.
                   *  @return   A pair of iterators pointing to the first mismatch.
                   *
                   *  This compares the elements of two ranges using the binary_pred
                   *  parameter, and returns a pair
                   *  of iterators.  The first iterator points into the first range, the
                   *  second iterator points into the second range, and the elements pointed
                   *  to by the iterators are not equal.
                  */
                  template<typename _InputIterator1, typename _InputIterator2,
                	   typename _BinaryPredicate>
                    pair<_InputIterator1, _InputIterator2>
                    mismatch(_InputIterator1 __first1, _InputIterator1 __last1,
                	     _InputIterator2 __first2, _BinaryPredicate __binary_pred)
                    {
                      // concept requirements
                      __glibcxx_function_requires(_InputIteratorConcept<_InputIterator1>)
                      __glibcxx_function_requires(_InputIteratorConcept<_InputIterator2>)
                      __glibcxx_requires_valid_range(__first1, __last1);
                
                      while (__first1 != __last1 && __binary_pred(*__first1, *__first2))
                        {
                	  ++__first1;
                	  ++__first2;
                        }
                      return pair<_InputIterator1, _InputIterator2>(__first1, __first2);
                    }
                
                  /**
                   *  @brief Tests a range for element-wise equality.
                   *  @param  first1  An input iterator.
                   *  @param  last1   An input iterator.
                   *  @param  first2  An input iterator.
                   *  @return   A boolean true or false.
                   *
                   *  This compares the elements of two ranges using @c == and returns true or
                   *  false depending on whether all of the corresponding elements of the
                   *  ranges are equal.
                  */
                  template<typename _InputIterator1, typename _InputIterator2>
                    inline bool
                    equal(_InputIterator1 __first1, _InputIterator1 __last1,
                	  _InputIterator2 __first2)
                    {
                      // concept requirements
                      __glibcxx_function_requires(_InputIteratorConcept<_InputIterator1>)
                      __glibcxx_function_requires(_InputIteratorConcept<_InputIterator2>)
                      __glibcxx_function_requires(_EqualOpConcept<
                	    typename iterator_traits<_InputIterator1>::value_type,
                	    typename iterator_traits<_InputIterator2>::value_type>)
                      __glibcxx_requires_valid_range(__first1, __last1);
                
                      for ( ; __first1 != __last1; ++__first1, ++__first2)
                	if (!(*__first1 == *__first2))
                	  return false;
                      return true;
                    }
                
                  /**
                   *  @brief Tests a range for element-wise equality.
                   *  @param  first1  An input iterator.
                   *  @param  last1   An input iterator.
                   *  @param  first2  An input iterator.
                   *  @param  binary_pred  A binary predicate @link s20_3_1_base functor@endlink.
                   *  @return   A boolean true or false.
                   *
                   *  This compares the elements of two ranges using the binary_pred
                   *  parameter, and returns true or
                   *  false depending on whether all of the corresponding elements of the
                   *  ranges are equal.
                  */
                  template<typename _InputIterator1, typename _InputIterator2,
                	   typename _BinaryPredicate>
                    inline bool
                    equal(_InputIterator1 __first1, _InputIterator1 __last1,
                	  _InputIterator2 __first2,
                	  _BinaryPredicate __binary_pred)
                    {
                      // concept requirements
                      __glibcxx_function_requires(_InputIteratorConcept<_InputIterator1>)
                      __glibcxx_function_requires(_InputIteratorConcept<_InputIterator2>)
                      __glibcxx_requires_valid_range(__first1, __last1);
                
                      for ( ; __first1 != __last1; ++__first1, ++__first2)
                	if (!__binary_pred(*__first1, *__first2))
                	  return false;
                      return true;
                    }
                
                  /**
                   *  @brief Performs "dictionary" comparison on ranges.
                   *  @param  first1  An input iterator.
                   *  @param  last1   An input iterator.
                   *  @param  first2  An input iterator.
                   *  @param  last2   An input iterator.
                   *  @return   A boolean true or false.
                   *
                   *  "Returns true if the sequence of elements defined by the range
                   *  [first1,last1) is lexicographically less than the sequence of elements
                   *  defined by the range [first2,last2).  Returns false otherwise."
                   *  (Quoted from [25.3.8]/1.)  If the iterators are all character pointers,
                   *  then this is an inline call to @c memcmp.
                  */
                  template<typename _InputIterator1, typename _InputIterator2>
                    bool
                    lexicographical_compare(_InputIterator1 __first1, _InputIterator1 __last1,
                			    _InputIterator2 __first2, _InputIterator2 __last2)
                    {
                      // concept requirements
                      __glibcxx_function_requires(_InputIteratorConcept<_InputIterator1>)
                      __glibcxx_function_requires(_InputIteratorConcept<_InputIterator2>)
                      __glibcxx_function_requires(_LessThanOpConcept<
                	    typename iterator_traits<_InputIterator1>::value_type,
                	    typename iterator_traits<_InputIterator2>::value_type>)
                      __glibcxx_function_requires(_LessThanOpConcept<
                	    typename iterator_traits<_InputIterator2>::value_type,
                	    typename iterator_traits<_InputIterator1>::value_type>)
                      __glibcxx_requires_valid_range(__first1, __last1);
                      __glibcxx_requires_valid_range(__first2, __last2);
                
                      for (;__first1 != __last1 && __first2 != __last2; ++__first1, ++__first2)
                	{
                	  if (*__first1 < *__first2)
                	    return true;
                	  if (*__first2 < *__first1)
                	    return false;
                	}
                      return __first1 == __last1 && __first2 != __last2;
                    }
                
                  /**
                   *  @brief Performs "dictionary" comparison on ranges.
                   *  @param  first1  An input iterator.
                   *  @param  last1   An input iterator.
                   *  @param  first2  An input iterator.
                   *  @param  last2   An input iterator.
                   *  @param  comp  A @link s20_3_3_comparisons comparison functor@endlink.
                   *  @return   A boolean true or false.
                   *
                   *  The same as the four-parameter @c lexigraphical_compare, but uses the
                   *  comp parameter instead of @c <.
                  */
                  template<typename _InputIterator1, typename _InputIterator2,
                	   typename _Compare>
                    bool
                    lexicographical_compare(_InputIterator1 __first1, _InputIterator1 __last1,
                			    _InputIterator2 __first2, _InputIterator2 __last2,
                			    _Compare __comp)
                    {
                      // concept requirements
                      __glibcxx_function_requires(_InputIteratorConcept<_InputIterator1>)
                      __glibcxx_function_requires(_InputIteratorConcept<_InputIterator2>)
                      __glibcxx_requires_valid_range(__first1, __last1);
                      __glibcxx_requires_valid_range(__first2, __last2);
                
                      for ( ; __first1 != __last1 && __first2 != __last2
                	    ; ++__first1, ++__first2)
                	{
                	  if (__comp(*__first1, *__first2))
                	    return true;
                	  if (__comp(*__first2, *__first1))
                	    return false;
                	}
                      return __first1 == __last1 && __first2 != __last2;
                    }
                
                  inline bool
                  lexicographical_compare(const unsigned char* __first1,
                			  const unsigned char* __last1,
                			  const unsigned char* __first2,
                			  const unsigned char* __last2)
                  {
                    __glibcxx_requires_valid_range(__first1, __last1);
                    __glibcxx_requires_valid_range(__first2, __last2);
                
                    const size_t __len1 = __last1 - __first1;
                    const size_t __len2 = __last2 - __first2;
                    const int __result = std::memcmp(__first1, __first2,
                				     std::min(__len1, __len2));
                    return __result != 0 ? __result < 0 : __len1 < __len2;
                  }
                
                  inline bool
                  lexicographical_compare(const char* __first1, const char* __last1,
                			  const char* __first2, const char* __last2)
                  {
                    __glibcxx_requires_valid_range(__first1, __last1);
                    __glibcxx_requires_valid_range(__first2, __last2);
                
                #if CHAR_MAX == SCHAR_MAX
                    return std::lexicographical_compare((const signed char*) __first1,
                					(const signed char*) __last1,
                					(const signed char*) __first2,
                					(const signed char*) __last2);
                #else /* CHAR_MAX == SCHAR_MAX */
                    return std::lexicographical_compare((const unsigned char*) __first1,
                					(const unsigned char*) __last1,
                					(const unsigned char*) __first2,
                					(const unsigned char*) __last2);
                #endif /* CHAR_MAX == SCHAR_MAX */
                  }
                
                } // namespace std
                
                #endif


Top 10 Lines:

     Line      Count

      517          1

Execution Summary:

        2   Executable lines in this file
        2   Lines executed
   100.00   Percent of the file executed

        1   Total number of line executions
     0.50   Average executions per line


*** File /cygdrive/c/Users/Paul/Documents/My Dropbox/Online Resources/My Homework/UBC 2011W/NSERC/matrix-factor/./Eigen/src/SparseCore/SparseMatrix.h:
                // This file is part of Eigen, a lightweight C++ template library
                // for linear algebra.
                //
                // Copyright (C) 2008-2010 Gael Guennebaud <gael.guennebaud@inria.fr>
                //
                // Eigen is free software; you can redistribute it and/or
                // modify it under the terms of the GNU Lesser General Public
                // License as published by the Free Software Foundation; either
                // version 3 of the License, or (at your option) any later version.
                //
                // Alternatively, you can redistribute it and/or
                // modify it under the terms of the GNU General Public License as
                // published by the Free Software Foundation; either version 2 of
                // the License, or (at your option) any later version.
                //
                // Eigen is distributed in the hope that it will be useful, but WITHOUT ANY
                // WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
                // FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License or the
                // GNU General Public License for more details.
                //
                // You should have received a copy of the GNU Lesser General Public
                // License and a copy of the GNU General Public License along with
                // Eigen. If not, see <http://www.gnu.org/licenses/>.
                
                #ifndef EIGEN_SPARSEMATRIX_H
                #define EIGEN_SPARSEMATRIX_H
                
                /** \ingroup SparseCore_Module
                  *
                  * \class SparseMatrix
                  *
                  * \brief A versatible sparse matrix representation
                  *
                  * This class implements a more versatile variants of the common \em compressed row/column storage format.
                  * Each colmun's (resp. row) non zeros are stored as a pair of value with associated row (resp. colmiun) index.
                  * All the non zeros are stored in a single large buffer. Unlike the \em compressed format, there might be extra
                  * space inbetween the nonzeros of two successive colmuns (resp. rows) such that insertion of new non-zero
                  * can be done with limited memory reallocation and copies.
                  *
                  * A call to the function makeCompressed() turns the matrix into the standard \em compressed format
                  * compatible with many library.
                  *
                  * More details on this storage sceheme are given in the \ref TutorialSparse "manual pages".
                  *
                  * \tparam _Scalar the scalar type, i.e. the type of the coefficients
                  * \tparam _Options Union of bit flags controlling the storage scheme. Currently the only possibility
                  *                 is RowMajor. The default is 0 which means column-major.
                  * \tparam _Index the type of the indices. It has to be a \b signed type (e.g., short, int, std::ptrdiff_t). Default is \c int.
                  *
                  * This class can be extended with the help of the plugin mechanism described on the page
                  * \ref TopicCustomizingEigen by defining the preprocessor symbol \c EIGEN_SPARSEMATRIX_PLUGIN.
                  */
                
                namespace internal {
                template<typename _Scalar, int _Options, typename _Index>
                struct traits<SparseMatrix<_Scalar, _Options, _Index> >
                {
                  typedef _Scalar Scalar;
                  typedef _Index Index;
                  typedef Sparse StorageKind;
                  typedef MatrixXpr XprKind;
                  enum {
                    RowsAtCompileTime = Dynamic,
                    ColsAtCompileTime = Dynamic,
                    MaxRowsAtCompileTime = Dynamic,
                    MaxColsAtCompileTime = Dynamic,
                    Flags = _Options | NestByRefBit | LvalueBit,
                    CoeffReadCost = NumTraits<Scalar>::ReadCost,
                    SupportedAccessPatterns = InnerRandomAccessPattern
                  };
                };
                
                template<typename _Scalar, int _Options, typename _Index, int DiagIndex>
                struct traits<Diagonal<const SparseMatrix<_Scalar, _Options, _Index>, DiagIndex> >
                {
                  typedef SparseMatrix<_Scalar, _Options, _Index> MatrixType;
                  typedef typename nested<MatrixType>::type MatrixTypeNested;
                  typedef typename remove_reference<MatrixTypeNested>::type _MatrixTypeNested;
                
                  typedef _Scalar Scalar;
                  typedef Dense StorageKind;
                  typedef _Index Index;
                  typedef MatrixXpr XprKind;
                
                  enum {
                    RowsAtCompileTime = Dynamic,
                    ColsAtCompileTime = 1,
                    MaxRowsAtCompileTime = Dynamic,
                    MaxColsAtCompileTime = 1,
                    Flags = 0,
                    CoeffReadCost = _MatrixTypeNested::CoeffReadCost*10
                  };
                };
                
                } // end namespace internal
                
                template<typename _Scalar, int _Options, typename _Index>
                class SparseMatrix
                  : public SparseMatrixBase<SparseMatrix<_Scalar, _Options, _Index> >
                {
                  public:
                    EIGEN_SPARSE_PUBLIC_INTERFACE(SparseMatrix)
                    EIGEN_SPARSE_INHERIT_ASSIGNMENT_OPERATOR(SparseMatrix, +=)
                    EIGEN_SPARSE_INHERIT_ASSIGNMENT_OPERATOR(SparseMatrix, -=)
                
                    typedef MappedSparseMatrix<Scalar,Flags> Map;
                    using Base::IsRowMajor;
                    typedef internal::CompressedStorage<Scalar,Index> Storage;
                    enum {
                      Options = _Options
                    };
                
                  protected:
                
                    typedef SparseMatrix<Scalar,(Flags&~RowMajorBit)|(IsRowMajor?RowMajorBit:0)> TransposedSparseMatrix;
                
                    Index m_outerSize;
                    Index m_innerSize;
                    Index* m_outerIndex;
                    Index* m_innerNonZeros;     // optional, if null then the data is compressed
                    Storage m_data;
                    
                    Eigen::Map<Matrix<Index,Dynamic,1> > innerNonZeros() { return Eigen::Map<Matrix<Index,Dynamic,1> >(m_innerNonZeros, m_innerNonZeros?m_outerSize:0); }
                    const  Eigen::Map<const Matrix<Index,Dynamic,1> > innerNonZeros() const { return Eigen::Map<const Matrix<Index,Dynamic,1> >(m_innerNonZeros, m_innerNonZeros?m_outerSize:0); }
                
                  public:
                    
                    /** \returns whether \c *this is in compressed form. */
                    inline bool isCompressed() const { return m_innerNonZeros==0; }
                
                    /** \returns the number of rows of the matrix */
                    inline Index rows() const { return IsRowMajor ? m_outerSize : m_innerSize; }
                    /** \returns the number of columns of the matrix */
                    inline Index cols() const { return IsRowMajor ? m_innerSize : m_outerSize; }
                
                    /** \returns the number of rows (resp. columns) of the matrix if the storage order column major (resp. row major) */
                    inline Index innerSize() const { return m_innerSize; }
                    /** \returns the number of columns (resp. rows) of the matrix if the storage order column major (resp. row major) */
                    inline Index outerSize() const { return m_outerSize; }
                    
                    /** \returns a const pointer to the array of values.
                      * This function is aimed at interoperability with other libraries.
                      * \sa innerIndexPtr(), outerIndexPtr() */
                    inline const Scalar* valuePtr() const { return &m_data.value(0); }
                    /** \returns a non-const pointer to the array of values.
                      * This function is aimed at interoperability with other libraries.
                      * \sa innerIndexPtr(), outerIndexPtr() */
                    inline Scalar* valuePtr() { return &m_data.value(0); }
                
                    /** \returns a const pointer to the array of inner indices.
                      * This function is aimed at interoperability with other libraries.
                      * \sa valuePtr(), outerIndexPtr() */
                    inline const Index* innerIndexPtr() const { return &m_data.index(0); }
                    /** \returns a non-const pointer to the array of inner indices.
                      * This function is aimed at interoperability with other libraries.
                      * \sa valuePtr(), outerIndexPtr() */
                    inline Index* innerIndexPtr() { return &m_data.index(0); }
                
                    /** \returns a const pointer to the array of the starting positions of the inner vectors.
                      * This function is aimed at interoperability with other libraries.
                      * \sa valuePtr(), innerIndexPtr() */
                    inline const Index* outerIndexPtr() const { return m_outerIndex; }
                    /** \returns a non-const pointer to the array of the starting positions of the inner vectors.
                      * This function is aimed at interoperability with other libraries.
                      * \sa valuePtr(), innerIndexPtr() */
                    inline Index* outerIndexPtr() { return m_outerIndex; }
                
                    /** \returns a const pointer to the array of the number of non zeros of the inner vectors.
                      * This function is aimed at interoperability with other libraries.
                      * \warning it returns the null pointer 0 in compressed mode */
                    inline const Index* innerNonZeroPtr() const { return m_innerNonZeros; }
                    /** \returns a non-const pointer to the array of the number of non zeros of the inner vectors.
                      * This function is aimed at interoperability with other libraries.
                      * \warning it returns the null pointer 0 in compressed mode */
                    inline Index* innerNonZeroPtr() { return m_innerNonZeros; }
                
                    /** \internal */
                    inline Storage& data() { return m_data; }
                    /** \internal */
                    inline const Storage& data() const { return m_data; }
                
                    /** \returns the value of the matrix at position \a i, \a j
                      * This function returns Scalar(0) if the element is an explicit \em zero */
                    inline Scalar coeff(Index row, Index col) const
                    {
                      const Index outer = IsRowMajor ? row : col;
                      const Index inner = IsRowMajor ? col : row;
                      Index end = m_innerNonZeros ? m_outerIndex[outer] + m_innerNonZeros[outer] : m_outerIndex[outer+1];
                      return m_data.atInRange(m_outerIndex[outer], end, inner);
                    }
                
                    /** \returns a non-const reference to the value of the matrix at position \a i, \a j
                      *
                      * If the element does not exist then it is inserted via the insert(Index,Index) function
                      * which itself turns the matrix into a non compressed form if that was not the case.
                      *
                      * This is a O(log(nnz_j)) operation (binary search) plus the cost of insert(Index,Index)
                      * function if the element does not already exist.
                      */
                    inline Scalar& coeffRef(Index row, Index col)
                    {
                      const Index outer = IsRowMajor ? row : col;
                      const Index inner = IsRowMajor ? col : row;
                
                      Index start = m_outerIndex[outer];
                      Index end = m_innerNonZeros ? m_outerIndex[outer] + m_innerNonZeros[outer] : m_outerIndex[outer+1];
                      eigen_assert(end>=start && "you probably called coeffRef on a non finalized matrix");
                      if(end<=start)
                        return insert(row,col);
                      const Index p = m_data.searchLowerIndex(start,end-1,inner);
                      if((p<end) && (m_data.index(p)==inner))
                        return m_data.value(p);
                      else
                        return insert(row,col);
                    }
                
                    /** \returns a reference to a novel non zero coefficient with coordinates \a row x \a col.
                      * The non zero coefficient must \b not already exist.
                      *
                      * If the matrix \c *this is in compressed mode, then \c *this is turned into uncompressed
                      * mode while reserving room for 2 non zeros per inner vector. It is strongly recommended to first
                      * call reserve(const SizesType &) to reserve a more appropriate number of elements per
                      * inner vector that better match your scenario.
                      *
                      * This function performs a sorted insertion in O(1) if the elements of each inner vector are
                      * inserted in increasing inner index order, and in O(nnz_j) for a random insertion.
                      *
                      */
                    EIGEN_DONT_INLINE Scalar& insert(Index row, Index col)
                    {
                      if(isCompressed())
                      {
                        reserve(VectorXi::Constant(outerSize(), 2));
                      }
                      return insertUncompressed(row,col);
                    }
                
                  public:
                
                    class InnerIterator;
                    class ReverseInnerIterator;
                
                    /** Removes all non zeros but keep allocated memory */
                    inline void setZero()
                    {
                      m_data.clear();
                      memset(m_outerIndex, 0, (m_outerSize+1)*sizeof(Index));
                      if(m_innerNonZeros)
                        memset(m_innerNonZeros, 0, (m_outerSize)*sizeof(Index));
                    }
                
                    /** \returns the number of non zero coefficients */
                    inline Index nonZeros() const
                    {
                      if(m_innerNonZeros)
                        return innerNonZeros().sum();
                      return static_cast<Index>(m_data.size());
                    }
                
                    /** Preallocates \a reserveSize non zeros.
                      *
                      * Precondition: the matrix must be in compressed mode. */
                    inline void reserve(Index reserveSize)
                    {
                      eigen_assert(isCompressed() && "This function does not make sense in non compressed mode.");
                      m_data.reserve(reserveSize);
                    }
                    
                    #ifdef EIGEN_PARSED_BY_DOXYGEN
                    /** Preallocates \a reserveSize[\c j] non zeros for each column (resp. row) \c j.
                      *
                      * This function turns the matrix in non-compressed mode */
                    template<class SizesType>
                    inline void reserve(const SizesType& reserveSizes);
                    #else
                    template<class SizesType>
                    inline void reserve(const SizesType& reserveSizes, const typename SizesType::value_type& enableif = typename SizesType::value_type())
                    {
                      EIGEN_UNUSED_VARIABLE(enableif);
                      reserveInnerVectors(reserveSizes);
                    }
                    template<class SizesType>
                    inline void reserve(const SizesType& reserveSizes, const typename SizesType::Scalar& enableif = typename SizesType::Scalar())
                    {
                      EIGEN_UNUSED_VARIABLE(enableif);
                      reserveInnerVectors(reserveSizes);
                    }
                    #endif // EIGEN_PARSED_BY_DOXYGEN
                  protected:
                    template<class SizesType>
                    inline void reserveInnerVectors(const SizesType& reserveSizes)
                    {
                      
                      if(isCompressed())
                      {
                        std::size_t totalReserveSize = 0;
                        // turn the matrix into non-compressed mode
                        m_innerNonZeros = new Index[m_outerSize];
                        
                        // temporarily use m_innerSizes to hold the new starting points.
                        Index* newOuterIndex = m_innerNonZeros;
                        
                        Index count = 0;
                        for(Index j=0; j<m_outerSize; ++j)
                        {
                          newOuterIndex[j] = count;
                          count += reserveSizes[j] + (m_outerIndex[j+1]-m_outerIndex[j]);
                          totalReserveSize += reserveSizes[j];
                        }
                        m_data.reserve(totalReserveSize);
                        std::ptrdiff_t previousOuterIndex = m_outerIndex[m_outerSize];
                        for(std::ptrdiff_t j=m_outerSize-1; j>=0; --j)
                        {
                          ptrdiff_t innerNNZ = previousOuterIndex - m_outerIndex[j];
                          for(std::ptrdiff_t i=innerNNZ-1; i>=0; --i)
                          {
                            m_data.index(newOuterIndex[j]+i) = m_data.index(m_outerIndex[j]+i);
                            m_data.value(newOuterIndex[j]+i) = m_data.value(m_outerIndex[j]+i);
                          }
                          previousOuterIndex = m_outerIndex[j];
                          m_outerIndex[j] = newOuterIndex[j];
                          m_innerNonZeros[j] = innerNNZ;
                        }
                        m_outerIndex[m_outerSize] = m_outerIndex[m_outerSize-1] + m_innerNonZeros[m_outerSize-1] + reserveSizes[m_outerSize-1];
                        
                        m_data.resize(m_outerIndex[m_outerSize]);
                      }
                      else
                      {
                        Index* newOuterIndex = new Index[m_outerSize+1];
                        Index count = 0;
                        for(Index j=0; j<m_outerSize; ++j)
                        {
                          newOuterIndex[j] = count;
                          Index alreadyReserved = (m_outerIndex[j+1]-m_outerIndex[j]) - m_innerNonZeros[j];
                          Index toReserve = std::max<std::ptrdiff_t>(reserveSizes[j], alreadyReserved);
                          count += toReserve + m_innerNonZeros[j];
                        }
                        newOuterIndex[m_outerSize] = count;
                        
                        m_data.resize(count);
                        for(ptrdiff_t j=m_outerSize-1; j>=0; --j)
                        {
                          std::ptrdiff_t offset = newOuterIndex[j] - m_outerIndex[j];
                          if(offset>0)
                          {
                            std::ptrdiff_t innerNNZ = m_innerNonZeros[j];
                            for(std::ptrdiff_t i=innerNNZ-1; i>=0; --i)
                            {
                              m_data.index(newOuterIndex[j]+i) = m_data.index(m_outerIndex[j]+i);
                              m_data.value(newOuterIndex[j]+i) = m_data.value(m_outerIndex[j]+i);
                            }
                          }
                        }
                        
                        std::swap(m_outerIndex, newOuterIndex);
                        delete[] newOuterIndex;
                      }
                      
                    }
                  public:
                
                    //--- low level purely coherent filling ---
                
                    /** \internal
                      * \returns a reference to the non zero coefficient at position \a row, \a col assuming that:
                      * - the nonzero does not already exist
                      * - the new coefficient is the last one according to the storage order
                      *
                      * Before filling a given inner vector you must call the statVec(Index) function.
                      *
                      * After an insertion session, you should call the finalize() function.
                      *
                      * \sa insert, insertBackByOuterInner, startVec */
                    inline Scalar& insertBack(Index row, Index col)
                    {
                      return insertBackByOuterInner(IsRowMajor?row:col, IsRowMajor?col:row);
                    }
                
                    /** \internal
                      * \sa insertBack, startVec */
                    inline Scalar& insertBackByOuterInner(Index outer, Index inner)
                    {
                      eigen_assert(size_t(m_outerIndex[outer+1]) == m_data.size() && "Invalid ordered insertion (invalid outer index)");
                      eigen_assert( (m_outerIndex[outer+1]-m_outerIndex[outer]==0 || m_data.index(m_data.size()-1)<inner) && "Invalid ordered insertion (invalid inner index)");
                      Index p = m_outerIndex[outer+1];
                      ++m_outerIndex[outer+1];
                      m_data.append(0, inner);
                      return m_data.value(p);
                    }
                
                    /** \internal
                      * \warning use it only if you know what you are doing */
                    inline Scalar& insertBackByOuterInnerUnordered(Index outer, Index inner)
                    {
                      Index p = m_outerIndex[outer+1];
                      ++m_outerIndex[outer+1];
                      m_data.append(0, inner);
                      return m_data.value(p);
                    }
                
                    /** \internal
                      * \sa insertBack, insertBackByOuterInner */
                    inline void startVec(Index outer)
       ##### ->     {
                      eigen_assert(m_outerIndex[outer]==int(m_data.size()) && "You must call startVec for each inner vector sequentially");
                      eigen_assert(m_outerIndex[outer+1]==0 && "You must call startVec for each inner vector sequentially");
                      m_outerIndex[outer+1] = m_outerIndex[outer];
                    }
                
                    /** \internal
                      * Must be called after inserting a set of non zero entries using the low level compressed API.
                      */
                    inline void finalize()
                    {
                      if(isCompressed())
                      {
                        Index size = static_cast<Index>(m_data.size());
                        Index i = m_outerSize;
                        // find the last filled column
                        while (i>=0 && m_outerIndex[i]==0)
                          --i;
                        ++i;
                        while (i<=m_outerSize)
                        {
                          m_outerIndex[i] = size;
                          ++i;
                        }
                      }
                    }
                
                    //---
                
                    template<typename InputIterators>
                    void setFromTriplets(const InputIterators& begin, const InputIterators& end);
                
                    void sumupDuplicates();
                
                    //---
                    
                    /** \internal
                      * same as insert(Index,Index) except that the indices are given relative to the storage order */
                    EIGEN_DONT_INLINE Scalar& insertByOuterInner(Index j, Index i)
                    {
                      return insert(IsRowMajor ? j : i, IsRowMajor ? i : j);
                    }
                
                    /** Turns the matrix into the \em compressed format.
                      */
                    void makeCompressed()
                    {
                      if(isCompressed())
                        return;
                      
                      Index oldStart = m_outerIndex[1];
                      m_outerIndex[1] = m_innerNonZeros[0];
                      for(Index j=1; j<m_outerSize; ++j)
                      {
                        Index nextOldStart = m_outerIndex[j+1];
                        std::ptrdiff_t offset = oldStart - m_outerIndex[j];
                        if(offset>0)
                        {
                          for(Index k=0; k<m_innerNonZeros[j]; ++k)
                          {
                            m_data.index(m_outerIndex[j]+k) = m_data.index(oldStart+k);
                            m_data.value(m_outerIndex[j]+k) = m_data.value(oldStart+k);
                          }
                        }
                        m_outerIndex[j+1] = m_outerIndex[j] + m_innerNonZeros[j];
                        oldStart = nextOldStart;
                      }
                      delete[] m_innerNonZeros;
                      m_innerNonZeros = 0;
                      m_data.resize(m_outerIndex[m_outerSize]);
                      m_data.squeeze();
                    }
                
                    /** Suppresses all nonzeros which are \b much \b smaller \b than \a reference under the tolerence \a epsilon */
                    void prune(Scalar reference, RealScalar epsilon = NumTraits<RealScalar>::dummy_precision())
                    {
                      prune(default_prunning_func(reference,epsilon));
                    }
                    
                    /** Turns the matrix into compressed format, and suppresses all nonzeros which do not satisfy the predicate \a keep.
                      * The functor type \a KeepFunc must implement the following function:
                      * \code
                      * bool operator() (const Index& row, const Index& col, const Scalar& value) const;
                      * \endcode
                      * \sa prune(Scalar,RealScalar)
                      */
                    template<typename KeepFunc>
                    void prune(const KeepFunc& keep = KeepFunc())
                    {
                      // TODO optimize the uncompressed mode to avoid moving and allocating the data twice
                      // TODO also implement a unit test
                      makeCompressed();
                
                      Index k = 0;
                      for(Index j=0; j<m_outerSize; ++j)
                      {
                        Index previousStart = m_outerIndex[j];
                        m_outerIndex[j] = k;
                        Index end = m_outerIndex[j+1];
                        for(Index i=previousStart; i<end; ++i)
                        {
                          if(keep(IsRowMajor?j:m_data.index(i), IsRowMajor?m_data.index(i):j, m_data.value(i)))
                          {
                            m_data.value(k) = m_data.value(i);
                            m_data.index(k) = m_data.index(i);
                            ++k;
                          }
                        }
                      }
                      m_outerIndex[m_outerSize] = k;
                      m_data.resize(k,0);
                    }
                
                    /** Resizes the matrix to a \a rows x \a cols matrix and initializes it to zero.
                      * \sa resizeNonZeros(Index), reserve(), setZero()
                      */
                    void resize(Index rows, Index cols)
           7 ->     {
                      const Index outerSize = IsRowMajor ? rows : cols;
                      m_innerSize = IsRowMajor ? cols : rows;
                      m_data.clear();
                      if (m_outerSize != outerSize || m_outerSize==0)
                      {
                        delete[] m_outerIndex;
                        m_outerIndex = new Index [outerSize+1];
                        m_outerSize = outerSize;
                      }
                      if(m_innerNonZeros)
                      {
                        delete[] m_innerNonZeros;
                        m_innerNonZeros = 0;
                      }
                      memset(m_outerIndex, 0, (m_outerSize+1)*sizeof(Index));
                    }
                
                    /** \internal
                      * Resize the nonzero vector to \a size */
                    void resizeNonZeros(Index size)
                    {
                      // TODO remove this function
                      m_data.resize(size);
                    }
                
                    /** \returns a const expression of the diagonal coefficients */
                    const Diagonal<const SparseMatrix> diagonal() const { return *this; }
                
                    /** Default constructor yielding an empty \c 0 \c x \c 0 matrix */
                    inline SparseMatrix()
                      : m_outerSize(-1), m_innerSize(0), m_outerIndex(0), m_innerNonZeros(0)
                    {
                      check_template_parameters();
                      resize(0, 0);
                    }
                
                    /** Constructs a \a rows \c x \a cols empty matrix */
                    inline SparseMatrix(Index rows, Index cols)
                      : m_outerSize(0), m_innerSize(0), m_outerIndex(0), m_innerNonZeros(0)
                    {
                      check_template_parameters();
                      resize(rows, cols);
                    }
                
                    /** Constructs a sparse matrix from the sparse expression \a other */
                    template<typename OtherDerived>
                    inline SparseMatrix(const SparseMatrixBase<OtherDerived>& other)
                      : m_outerSize(0), m_innerSize(0), m_outerIndex(0), m_innerNonZeros(0)
                    {
                      check_template_parameters();
                      *this = other.derived();
                    }
                
                    /** Copy constructor (it performs a deep copy) */
                    inline SparseMatrix(const SparseMatrix& other)
                      : Base(), m_outerSize(0), m_innerSize(0), m_outerIndex(0), m_innerNonZeros(0)
                    {
                      check_template_parameters();
                      *this = other.derived();
                    }
                
                    /** Swaps the content of two sparse matrices of the same type.
                      * This is a fast operation that simply swaps the underlying pointers and parameters. */
                    inline void swap(SparseMatrix& other)
                    {
                      //EIGEN_DBG_SPARSE(std::cout << "SparseMatrix:: swap\n");
                      std::swap(m_outerIndex, other.m_outerIndex);
                      std::swap(m_innerSize, other.m_innerSize);
                      std::swap(m_outerSize, other.m_outerSize);
                      std::swap(m_innerNonZeros, other.m_innerNonZeros);
                      m_data.swap(other.m_data);
                    }
                
                    inline SparseMatrix& operator=(const SparseMatrix& other)
           4 ->     {
                      if (other.isRValue())
                      {
                        swap(other.const_cast_derived());
                      }
                      else
                      {
                        initAssignment(other);
                        if(other.isCompressed())
                        {
                          memcpy(m_outerIndex, other.m_outerIndex, (m_outerSize+1)*sizeof(Index));
                          m_data = other.m_data;
                        }
                        else
                        {
                          Base::operator=(other);
                        }
                      }
                      return *this;
                    }
                
                    #ifndef EIGEN_PARSED_BY_DOXYGEN
                    template<typename Lhs, typename Rhs>
                    inline SparseMatrix& operator=(const SparseSparseProduct<Lhs,Rhs>& product)
                    { return Base::operator=(product); }
                    
                    template<typename OtherDerived>
                    inline SparseMatrix& operator=(const ReturnByValue<OtherDerived>& other)
                    { return Base::operator=(other.derived()); }
                    
                    template<typename OtherDerived>
                    inline SparseMatrix& operator=(const EigenBase<OtherDerived>& other)
                    { return Base::operator=(other.derived()); }
                    #endif
                
                    template<typename OtherDerived>
                    EIGEN_DONT_INLINE SparseMatrix& operator=(const SparseMatrixBase<OtherDerived>& other)
           3 ->     {
                      initAssignment(other.derived());
                      const bool needToTranspose = (Flags & RowMajorBit) != (OtherDerived::Flags & RowMajorBit);
                      if (needToTranspose)
                      {
                        // two passes algorithm:
                        //  1 - compute the number of coeffs per dest inner vector
                        //  2 - do the actual copy/eval
                        // Since each coeff of the rhs has to be evaluated twice, let's evaluate it if needed
                        typedef typename internal::nested<OtherDerived,2>::type OtherCopy;
                        typedef typename internal::remove_all<OtherCopy>::type _OtherCopy;
                        OtherCopy otherCopy(other.derived());
                
                        Eigen::Map<Matrix<Index, Dynamic, 1> > (m_outerIndex,outerSize()).setZero();
                        // pass 1
                        // FIXME the above copy could be merged with that pass
                        for (Index j=0; j<otherCopy.outerSize(); ++j)
                          for (typename _OtherCopy::InnerIterator it(otherCopy, j); it; ++it)
                            ++m_outerIndex[it.index()];
                
                        // prefix sum
                        Index count = 0;
                        VectorXi positions(outerSize());
                        for (Index j=0; j<outerSize(); ++j)
                        {
                          Index tmp = m_outerIndex[j];
                          m_outerIndex[j] = count;
                          positions[j] = count;
                          count += tmp;
                        }
                        m_outerIndex[outerSize()] = count;
                        // alloc
                        m_data.resize(count);
                        // pass 2
                        for (Index j=0; j<otherCopy.outerSize(); ++j)
                        {
                          for (typename _OtherCopy::InnerIterator it(otherCopy, j); it; ++it)
                          {
                            Index pos = positions[it.index()]++;
                            m_data.index(pos) = j;
                            m_data.value(pos) = it.value();
                          }
                        }
                        return *this;
                      }
                      else
                      {
                        // there is no special optimization
                        return Base::operator=(other.derived());
                      }
                    }
                
                    friend std::ostream & operator << (std::ostream & s, const SparseMatrix& m)
                    {
                      EIGEN_DBG_SPARSE(
                        s << "Nonzero entries:\n";
                        if(m.isCompressed())
                          for (Index i=0; i<m.nonZeros(); ++i)
                            s << "(" << m.m_data.value(i) << "," << m.m_data.index(i) << ") ";
                        else
                          for (Index i=0; i<m.outerSize(); ++i)
                          {
                            int p = m.m_outerIndex[i];
                            int pe = m.m_outerIndex[i]+m.m_innerNonZeros[i];
                            Index k=p;
                            for (; k<pe; ++k)
                              s << "(" << m.m_data.value(k) << "," << m.m_data.index(k) << ") ";
                            for (; k<m.m_outerIndex[i+1]; ++k)
                              s << "(_,_) ";
                          }
                        s << std::endl;
                        s << std::endl;
                        s << "Outer pointers:\n";
                        for (Index i=0; i<m.outerSize(); ++i)
                          s << m.m_outerIndex[i] << " ";
                        s << " $" << std::endl;
                        if(!m.isCompressed())
                        {
                          s << "Inner non zeros:\n";
                          for (Index i=0; i<m.outerSize(); ++i)
                            s << m.m_innerNonZeros[i] << " ";
                          s << " $" << std::endl;
                        }
                        s << std::endl;
                      );
                      s << static_cast<const SparseMatrixBase<SparseMatrix>&>(m);
                      return s;
                    }
                
                    /** Destructor */
                    inline ~SparseMatrix()
                    {
                      delete[] m_outerIndex;
                      delete[] m_innerNonZeros;
                    }
                
                #ifndef EIGEN_PARSED_BY_DOXYGEN
                    /** Overloaded for performance */
                    Scalar sum() const;
                #endif
                    
                #   ifdef EIGEN_SPARSEMATRIX_PLUGIN
                #     include EIGEN_SPARSEMATRIX_PLUGIN
                #   endif
                
                protected:
                
                    template<typename Other>
                    void initAssignment(const Other& other)
                    {
                      resize(other.rows(), other.cols());
                      if(m_innerNonZeros)
                      {
                        delete[] m_innerNonZeros;
                        m_innerNonZeros = 0;
                      }
                    }
                
                    /** \internal
                      * \sa insert(Index,Index) */
                    EIGEN_DONT_INLINE Scalar& insertCompressed(Index row, Index col)
                    {
                      eigen_assert(isCompressed());
                
                      const Index outer = IsRowMajor ? row : col;
                      const Index inner = IsRowMajor ? col : row;
                
                      Index previousOuter = outer;
                      if (m_outerIndex[outer+1]==0)
                      {
                        // we start a new inner vector
                        while (previousOuter>=0 && m_outerIndex[previousOuter]==0)
                        {
                          m_outerIndex[previousOuter] = static_cast<Index>(m_data.size());
                          --previousOuter;
                        }
                        m_outerIndex[outer+1] = m_outerIndex[outer];
                      }
                
                      // here we have to handle the tricky case where the outerIndex array
                      // starts with: [ 0 0 0 0 0 1 ...] and we are inserted in, e.g.,
                      // the 2nd inner vector...
                      bool isLastVec = (!(previousOuter==-1 && m_data.size()!=0))
                                    && (size_t(m_outerIndex[outer+1]) == m_data.size());
                
                      size_t startId = m_outerIndex[outer];
                      // FIXME let's make sure sizeof(long int) == sizeof(size_t)
                      size_t p = m_outerIndex[outer+1];
                      ++m_outerIndex[outer+1];
                
                      float reallocRatio = 1;
                      if (m_data.allocatedSize()<=m_data.size())
                      {
                        // if there is no preallocated memory, let's reserve a minimum of 32 elements
                        if (m_data.size()==0)
                        {
                          m_data.reserve(32);
                        }
                        else
                        {
                          // we need to reallocate the data, to reduce multiple reallocations
                          // we use a smart resize algorithm based on the current filling ratio
                          // in addition, we use float to avoid integers overflows
                          float nnzEstimate = float(m_outerIndex[outer])*float(m_outerSize)/float(outer+1);
                          reallocRatio = (nnzEstimate-float(m_data.size()))/float(m_data.size());
                          // furthermore we bound the realloc ratio to:
                          //   1) reduce multiple minor realloc when the matrix is almost filled
                          //   2) avoid to allocate too much memory when the matrix is almost empty
                          reallocRatio = (std::min)((std::max)(reallocRatio,1.5f),8.f);
                        }
                      }
                      m_data.resize(m_data.size()+1,reallocRatio);
                
                      if (!isLastVec)
                      {
                        if (previousOuter==-1)
                        {
                          // oops wrong guess.
                          // let's correct the outer offsets
                          for (Index k=0; k<=(outer+1); ++k)
                            m_outerIndex[k] = 0;
                          Index k=outer+1;
                          while(m_outerIndex[k]==0)
                            m_outerIndex[k++] = 1;
                          while (k<=m_outerSize && m_outerIndex[k]!=0)
                            m_outerIndex[k++]++;
                          p = 0;
                          --k;
                          k = m_outerIndex[k]-1;
                          while (k>0)
                          {
                            m_data.index(k) = m_data.index(k-1);
                            m_data.value(k) = m_data.value(k-1);
                            k--;
                          }
                        }
                        else
                        {
                          // we are not inserting into the last inner vec
                          // update outer indices:
                          Index j = outer+2;
                          while (j<=m_outerSize && m_outerIndex[j]!=0)
                            m_outerIndex[j++]++;
                          --j;
                          // shift data of last vecs:
                          Index k = m_outerIndex[j]-1;
                          while (k>=Index(p))
                          {
                            m_data.index(k) = m_data.index(k-1);
                            m_data.value(k) = m_data.value(k-1);
                            k--;
                          }
                        }
                      }
                
                      while ( (p > startId) && (m_data.index(p-1) > inner) )
                      {
                        m_data.index(p) = m_data.index(p-1);
                        m_data.value(p) = m_data.value(p-1);
                        --p;
                      }
                
                      m_data.index(p) = inner;
                      return (m_data.value(p) = 0);
                    }
                
                    /** \internal
                      * A vector object that is equal to 0 everywhere but v at the position i */
                    class SingletonVector
                    {
                        Index m_index;
                        Index m_value;
                      public:
                        typedef Index value_type;
                        SingletonVector(Index i, Index v)
                          : m_index(i), m_value(v)
                        {}
                
                        Index operator[](Index i) const { return i==m_index ? m_value : 0; }
                    };
                
                    /** \internal
                      * \sa insert(Index,Index) */
                    EIGEN_DONT_INLINE Scalar& insertUncompressed(Index row, Index col)
       38312 ->     {
                      eigen_assert(!isCompressed());
                
                      const Index outer = IsRowMajor ? row : col;
                      const Index inner = IsRowMajor ? col : row;
                
                      std::ptrdiff_t room = m_outerIndex[outer+1] - m_outerIndex[outer];
                      std::ptrdiff_t innerNNZ = m_innerNonZeros[outer];
                      if(innerNNZ>=room)
                      {
                        // this inner vector is full, we need to reallocate the whole buffer :(
                        reserve(SingletonVector(outer,std::max<std::ptrdiff_t>(2,innerNNZ)));
                      }
                
                      Index startId = m_outerIndex[outer];
                      Index p = startId + m_innerNonZeros[outer];
                      while ( (p > startId) && (m_data.index(p-1) > inner) )
                      {
                        m_data.index(p) = m_data.index(p-1);
                        m_data.value(p) = m_data.value(p-1);
                        --p;
                      }
                
                      m_innerNonZeros[outer]++;
                
                      m_data.index(p) = inner;
                      return (m_data.value(p) = 0);
                    }
                
                public:
                    /** \internal
                      * \sa insert(Index,Index) */
                    inline Scalar& insertBackUncompressed(Index row, Index col)
                    {
                      const Index outer = IsRowMajor ? row : col;
                      const Index inner = IsRowMajor ? col : row;
                
                      eigen_assert(!isCompressed());
                      eigen_assert(m_innerNonZeros[outer]<=(m_outerIndex[outer+1] - m_outerIndex[outer]));
                
                      Index p = m_outerIndex[outer] + m_innerNonZeros[outer];
                      m_innerNonZeros[outer]++;
                      m_data.index(p) = inner;
                      return (m_data.value(p) = 0);
                    }
                
                private:
                  static void check_template_parameters()
                  {
                    EIGEN_STATIC_ASSERT(NumTraits<Index>::IsSigned,THE_INDEX_TYPE_MUST_BE_A_SIGNED_TYPE);
                  }
                
                  struct default_prunning_func {
                    default_prunning_func(Scalar ref, RealScalar eps) : reference(ref), epsilon(eps) {}
                    inline bool operator() (const Index&, const Index&, const Scalar& value) const
                    {
                      return !internal::isMuchSmallerThan(value, reference, epsilon);
                    }
                    Scalar reference;
                    RealScalar epsilon;
                  };
                };
                
                template<typename Scalar, int _Options, typename _Index>
                class SparseMatrix<Scalar,_Options,_Index>::InnerIterator
                {
                  public:
                    InnerIterator(const SparseMatrix& mat, Index outer)
                      : m_values(mat.valuePtr()), m_indices(mat.innerIndexPtr()), m_outer(outer), m_id(mat.m_outerIndex[outer])
                    {
                      if(mat.isCompressed())
                        m_end = mat.m_outerIndex[outer+1];
                      else
                        m_end = m_id + mat.m_innerNonZeros[outer];
                    }
                
                    inline InnerIterator& operator++() { m_id++; return *this; }
                
                    inline const Scalar& value() const { return m_values[m_id]; }
                    inline Scalar& valueRef() { return const_cast<Scalar&>(m_values[m_id]); }
                
                    inline Index index() const { return m_indices[m_id]; }
                    inline Index outer() const { return m_outer; }
                    inline Index row() const { return IsRowMajor ? m_outer : index(); }
                    inline Index col() const { return IsRowMajor ? index() : m_outer; }
                
                    inline operator bool() const { return (m_id < m_end); }
                
                  protected:
                    const Scalar* m_values;
                    const Index* m_indices;
                    const Index m_outer;
                    Index m_id;
                    Index m_end;
                };
                
                template<typename Scalar, int _Options, typename _Index>
                class SparseMatrix<Scalar,_Options,_Index>::ReverseInnerIterator
                {
                  public:
                    ReverseInnerIterator(const SparseMatrix& mat, Index outer)
                      : m_values(mat.valuePtr()), m_indices(mat.innerIndexPtr()), m_outer(outer), m_start(mat.m_outerIndex[outer])
                    {
                      if(mat.isCompressed())
                        m_id = mat.m_outerIndex[outer+1];
                      else
                        m_id = m_start + mat.m_innerNonZeros[outer];
                    }
                
                    inline ReverseInnerIterator& operator--() { --m_id; return *this; }
                
                    inline const Scalar& value() const { return m_values[m_id-1]; }
                    inline Scalar& valueRef() { return const_cast<Scalar&>(m_values[m_id-1]); }
                
                    inline Index index() const { return m_indices[m_id-1]; }
                    inline Index outer() const { return m_outer; }
                    inline Index row() const { return IsRowMajor ? m_outer : index(); }
                    inline Index col() const { return IsRowMajor ? index() : m_outer; }
                
                    inline operator bool() const { return (m_id > m_start); }
                
                  protected:
                    const Scalar* m_values;
                    const Index* m_indices;
                    const Index m_outer;
                    Index m_id;
                    const Index m_start;
                };
                
                namespace internal {
                
                template<typename InputIterator, typename SparseMatrixType>
                void set_from_triplets(const InputIterator& begin, const InputIterator& end, SparseMatrixType& mat, int Options = 0)
                {
                  EIGEN_UNUSED_VARIABLE(Options);
                  enum { IsRowMajor = SparseMatrixType::IsRowMajor };
                  typedef typename SparseMatrixType::Scalar Scalar;
                  typedef typename SparseMatrixType::Index Index;
                  SparseMatrix<Scalar,IsRowMajor?ColMajor:RowMajor> trMat(mat.rows(),mat.cols());
                
                  // pass 1: count the nnz per inner-vector
                  VectorXi wi(trMat.outerSize());
                  wi.setZero();
                  for(InputIterator it(begin); it!=end; ++it)
                    wi(IsRowMajor ? it->col() : it->row())++;
                
                  // pass 2: insert all the elements into trMat
                  trMat.reserve(wi);
                  for(InputIterator it(begin); it!=end; ++it)
                    trMat.insertBackUncompressed(it->row(),it->col()) = it->value();
                
                  // pass 3:
                  trMat.sumupDuplicates();
                
                  // pass 4: transposed copy -> implicit sorting
                  mat = trMat;
                }
                
                }
                
                
                /** Fill the matrix \c *this with the list of \em triplets defined by the iterator range \a begin - \b.
                  *
                  * A \em triplet is a tuple (i,j,value) defining a non-zero element.
                  * The input list of triplets does not have to be sorted, and can contains duplicated elements.
                  * In any case, the result is a \b sorted and \b compressed sparse matrix where the duplicates have been summed up.
                  * This is a \em O(n) operation, with \em n the number of triplet elements.
                  * The initial contents of \c *this is destroyed.
                  * The matrix \c *this must be properly resized beforehand using the SparseMatrix(Index,Index) constructor,
                  * or the resize(Index,Index) method. The sizes are not extracted from the triplet list.
                  *
                  * The \a InputIterators value_type must provide the following interface:
                  * \code
                  * Scalar value() const; // the value
                  * Scalar row() const;   // the row index i
                  * Scalar col() const;   // the column index j
                  * \endcode
                  * See for instance the Eigen::Triplet template class.
                  *
                  * Here is a typical usage example:
                  * \code
                    typedef Triplet<double> T;
                    std::vector<T> tripletList;
                    triplets.reserve(estimation_of_entries);
                    for(...)
                    {
                      // ...
                      tripletList.push_back(T(i,j,v_ij));
                    }
                    SparseMatrixType m(rows,cols);
                    m.setFromTriplets(tripletList.begin(), tripletList.end());
                    // m is ready to go!
                  * \endcode
                  *
                  * \warning The list of triplets is read multiple times (at least twice). Therefore, it is not recommended to define
                  * an abstract iterator over a complex data-structure that would be expensive to evaluate. The triplets should rather
                  * be explicitely stored into a std::vector for instance.
                  */
                template<typename Scalar, int _Options, typename _Index>
                template<typename InputIterators>
                void SparseMatrix<Scalar,_Options,_Index>::setFromTriplets(const InputIterators& begin, const InputIterators& end)
                {
                  internal::set_from_triplets(begin, end, *this);
                }
                
                /** \internal */
                template<typename Scalar, int _Options, typename _Index>
                void SparseMatrix<Scalar,_Options,_Index>::sumupDuplicates()
                {
                  eigen_assert(!isCompressed());
                  // TODO, in practice we should be able to use m_innerNonZeros for that task
                  VectorXi wi(innerSize());
                  wi.fill(-1);
                  Index count = 0;
                  // for each inner-vector, wi[inner_index] will hold the position of first element into the index/value buffers
                  for(int j=0; j<outerSize(); ++j)
                  {
                    Index start   = count;
                    Index oldEnd  = m_outerIndex[j]+m_innerNonZeros[j];
                    for(Index k=m_outerIndex[j]; k<oldEnd; ++k)
                    {
                      Index i = m_data.index(k);
                      if(wi(i)>=start)
                      {
                        // we already meet this entry => accumulate it
                        m_data.value(wi(i)) += m_data.value(k);
                      }
                      else
                      {
                        m_data.value(count) = m_data.value(k);
                        m_data.index(count) = m_data.index(k);
                        wi(i) = count;
                        ++count;
                      }
                    }
                    m_outerIndex[j] = start;
                  }
                  m_outerIndex[m_outerSize] = count;
                
                  // turn the matrix into compressed form
                  delete[] m_innerNonZeros;
                  m_innerNonZeros = 0;
                  m_data.resize(m_outerIndex[m_outerSize]);
                }
                
                #endif // EIGEN_SPARSEMATRIX_H


Top 10 Lines:

     Line      Count

      878      38312
      522          7
      597          4
      634          3

Execution Summary:

        5   Executable lines in this file
        5   Lines executed
   100.00   Percent of the file executed

    38326   Total number of line executions
  7665.20   Average executions per line


*** File /cygdrive/c/Users/Paul/Documents/My Dropbox/Online Resources/My Homework/UBC 2011W/NSERC/matrix-factor/ldl_driver.cpp:
                #include <iostream>
                #include <string>
                #include <Eigen/Sparse>
                #include <unsupported/Eigen/SparseExtra>
                #include <Crout.h>
                
                using namespace Eigen;
                using namespace std;
                
           2 -> int main(int argc, char* argv[]) {
                    if (argc != 3) {
                		cout << "Too many or too few arguments." << endl;
                		cout << "Program usage: ./ldl_driver [in.mtx] [out.mtx]" << endl;
                		return 0;
                	}
                	
                	SparseMatrix<double> A;
                	
                	loadMarket(A, argv[1]);
                	
                	SparseMatrix<double> L(A);
                	VectorXd D(A.rows());
                	
                	CroutLDL<double>(L, D, true);
                	
                	saveMarket(L, argv[2]);
                	
                	printf("The residual is %f.\n",((L * D.asDiagonal() * L.transpose()).triangularView<Lower>() - A).cwiseAbs().sum());
                	
                	return 0;
                }


Top 10 Lines:

     Line      Count

       10          2

Execution Summary:

        2   Executable lines in this file
        1   Lines executed
    50.00   Percent of the file executed

        2   Total number of line executions
     1.00   Average executions per line


*** File /cygdrive/c/Users/Paul/Documents/My Dropbox/Online Resources/My Homework/UBC 2011W/NSERC/matrix-factor/./Eigen/src/Core/util/Macros.h:
                // This file is part of Eigen, a lightweight C++ template library
                // for linear algebra.
                //
                // Copyright (C) 2008-2010 Gael Guennebaud <gael.guennebaud@inria.fr>
                // Copyright (C) 2006-2008 Benoit Jacob <jacob.benoit.1@gmail.com>
                //
                // Eigen is free software; you can redistribute it and/or
                // modify it under the terms of the GNU Lesser General Public
                // License as published by the Free Software Foundation; either
                // version 3 of the License, or (at your option) any later version.
                //
                // Alternatively, you can redistribute it and/or
                // modify it under the terms of the GNU General Public License as
                // published by the Free Software Foundation; either version 2 of
                // the License, or (at your option) any later version.
                //
                // Eigen is distributed in the hope that it will be useful, but WITHOUT ANY
                // WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
                // FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License or the
                // GNU General Public License for more details.
                //
                // You should have received a copy of the GNU Lesser General Public
                // License and a copy of the GNU General Public License along with
                // Eigen. If not, see <http://www.gnu.org/licenses/>.
                
                #ifndef EIGEN_MACROS_H
                #define EIGEN_MACROS_H
                
                #define EIGEN_WORLD_VERSION 3
                #define EIGEN_MAJOR_VERSION 0
                #define EIGEN_MINOR_VERSION 92
                
                #define EIGEN_VERSION_AT_LEAST(x,y,z) (EIGEN_WORLD_VERSION>x || (EIGEN_WORLD_VERSION>=x && \
                                                      (EIGEN_MAJOR_VERSION>y || (EIGEN_MAJOR_VERSION>=y && \
                                                                                 EIGEN_MINOR_VERSION>=z))))
                #ifdef __GNUC__
                  #define EIGEN_GNUC_AT_LEAST(x,y) ((__GNUC__==x && __GNUC_MINOR__>=y) || __GNUC__>x)
                #else
                  #define EIGEN_GNUC_AT_LEAST(x,y) 0
                #endif
                 
                #ifdef __GNUC__
                  #define EIGEN_GNUC_AT_MOST(x,y) ((__GNUC__==x && __GNUC_MINOR__<=y) || __GNUC__<x)
                #else
                  #define EIGEN_GNUC_AT_MOST(x,y) 0
                #endif
                
                #if EIGEN_GNUC_AT_MOST(4,3) && !defined(__clang__)
                  // see bug 89
                  #define EIGEN_SAFE_TO_USE_STANDARD_ASSERT_MACRO 0
                #else
                  #define EIGEN_SAFE_TO_USE_STANDARD_ASSERT_MACRO 1
                #endif
                
                #if defined(__GNUC__) && (__GNUC__ <= 3)
                #define EIGEN_GCC3_OR_OLDER 1
                #else
                #define EIGEN_GCC3_OR_OLDER 0
                #endif
                
                // 16 byte alignment is only useful for vectorization. Since it affects the ABI, we need to enable
                // 16 byte alignment on all platforms where vectorization might be enabled. In theory we could always
                // enable alignment, but it can be a cause of problems on some platforms, so we just disable it in
                // certain common platform (compiler+architecture combinations) to avoid these problems.
                // Only static alignment is really problematic (relies on nonstandard compiler extensions that don't
                // work everywhere, for example don't work on GCC/ARM), try to keep heap alignment even
                // when we have to disable static alignment.
                #if defined(__GNUC__) && !(defined(__i386__) || defined(__x86_64__) || defined(__powerpc__) || defined(__ppc__) || defined(__ia64__))
                #define EIGEN_GCC_AND_ARCH_DOESNT_WANT_STACK_ALIGNMENT 1
                #else
                #define EIGEN_GCC_AND_ARCH_DOESNT_WANT_STACK_ALIGNMENT 0
                #endif
                
                // static alignment is completely disabled with GCC 3, Sun Studio, and QCC/QNX
                #if !EIGEN_GCC_AND_ARCH_DOESNT_WANT_STACK_ALIGNMENT \
                 && !EIGEN_GCC3_OR_OLDER \
                 && !defined(__SUNPRO_CC) \
                 && !defined(__QNXNTO__)
                  #define EIGEN_ARCH_WANTS_STACK_ALIGNMENT 1
                #else
                  #define EIGEN_ARCH_WANTS_STACK_ALIGNMENT 0
                #endif
                
                #ifdef EIGEN_DONT_ALIGN
                  #ifndef EIGEN_DONT_ALIGN_STATICALLY
                    #define EIGEN_DONT_ALIGN_STATICALLY
                  #endif
                  #define EIGEN_ALIGN 0
                #else
                  #define EIGEN_ALIGN 1
                #endif
                
                // EIGEN_ALIGN_STATICALLY is the true test whether we want to align arrays on the stack or not. It takes into account both the user choice to explicitly disable
                // alignment (EIGEN_DONT_ALIGN_STATICALLY) and the architecture config (EIGEN_ARCH_WANTS_STACK_ALIGNMENT). Henceforth, only EIGEN_ALIGN_STATICALLY should be used.
                #if EIGEN_ARCH_WANTS_STACK_ALIGNMENT && !defined(EIGEN_DONT_ALIGN_STATICALLY)
                  #define EIGEN_ALIGN_STATICALLY 1
                #else
                  #define EIGEN_ALIGN_STATICALLY 0
                  #ifndef EIGEN_DISABLE_UNALIGNED_ARRAY_ASSERT
                    #define EIGEN_DISABLE_UNALIGNED_ARRAY_ASSERT
                  #endif
                #endif
                
                #ifdef EIGEN_DEFAULT_TO_ROW_MAJOR
                #define EIGEN_DEFAULT_MATRIX_STORAGE_ORDER_OPTION RowMajor
                #else
                #define EIGEN_DEFAULT_MATRIX_STORAGE_ORDER_OPTION ColMajor
                #endif
                
                #ifndef EIGEN_DEFAULT_DENSE_INDEX_TYPE
                #define EIGEN_DEFAULT_DENSE_INDEX_TYPE std::ptrdiff_t
                #endif
                
                /** Allows to disable some optimizations which might affect the accuracy of the result.
                  * Such optimization are enabled by default, and set EIGEN_FAST_MATH to 0 to disable them.
                  * They currently include:
                  *   - single precision Cwise::sin() and Cwise::cos() when SSE vectorization is enabled.
                  */
                #ifndef EIGEN_FAST_MATH
                #define EIGEN_FAST_MATH 1
                #endif
                
                #define EIGEN_DEBUG_VAR(x) std::cerr << #x << " = " << x << std::endl;
                
                // concatenate two tokens
                #define EIGEN_CAT2(a,b) a ## b
                #define EIGEN_CAT(a,b) EIGEN_CAT2(a,b)
                
                // convert a token to a string
                #define EIGEN_MAKESTRING2(a) #a
                #define EIGEN_MAKESTRING(a) EIGEN_MAKESTRING2(a)
                
                #if EIGEN_GNUC_AT_LEAST(4,1) && !defined(__clang__) && !defined(__INTEL_COMPILER)
                #define EIGEN_FLATTEN_ATTRIB __attribute__((flatten))
                #else
                #define EIGEN_FLATTEN_ATTRIB
                #endif
                
                // EIGEN_STRONG_INLINE is a stronger version of the inline, using __forceinline on MSVC,
                // but it still doesn't use GCC's always_inline. This is useful in (common) situations where MSVC needs forceinline
                // but GCC is still doing fine with just inline.
                #if (defined _MSC_VER) || (defined __INTEL_COMPILER)
                #define EIGEN_STRONG_INLINE __forceinline
                #else
                #define EIGEN_STRONG_INLINE inline
                #endif
                
                // EIGEN_ALWAYS_INLINE is the stronget, it has the effect of making the function inline and adding every possible
                // attribute to maximize inlining. This should only be used when really necessary: in particular,
                // it uses __attribute__((always_inline)) on GCC, which most of the time is useless and can severely harm compile times.
                // FIXME with the always_inline attribute,
                // gcc 3.4.x reports the following compilation error:
                //   Eval.h:91: sorry, unimplemented: inlining failed in call to 'const Eigen::Eval<Derived> Eigen::MatrixBase<Scalar, Derived>::eval() const'
                //    : function body not available
                #if EIGEN_GNUC_AT_LEAST(4,0)
                #define EIGEN_ALWAYS_INLINE __attribute__((always_inline)) inline
                #else
                #define EIGEN_ALWAYS_INLINE EIGEN_STRONG_INLINE
                #endif
                
                #if (defined __GNUC__)
                #define EIGEN_DONT_INLINE __attribute__((noinline))
                #elif (defined _MSC_VER)
                #define EIGEN_DONT_INLINE __declspec(noinline)
                #else
                #define EIGEN_DONT_INLINE
                #endif
                
                // this macro allows to get rid of linking errors about multiply defined functions.
                //  - static is not very good because it prevents definitions from different object files to be merged.
                //           So static causes the resulting linked executable to be bloated with multiple copies of the same function.
                //  - inline is not perfect either as it unwantedly hints the compiler toward inlining the function.
                #define EIGEN_DECLARE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS
                #define EIGEN_DEFINE_FUNCTION_ALLOWING_MULTIPLE_DEFINITIONS inline
                
                #ifdef NDEBUG
                # ifndef EIGEN_NO_DEBUG
                #  define EIGEN_NO_DEBUG
                # endif
                #endif
                
                // eigen_plain_assert is where we implement the workaround for the assert() bug in GCC <= 4.3, see bug 89
                #ifdef EIGEN_NO_DEBUG
                  #define eigen_plain_assert(x)
                #else
                  #if EIGEN_SAFE_TO_USE_STANDARD_ASSERT_MACRO
                    namespace Eigen {
                    namespace internal {
                    inline bool copy_bool(bool b) { return b; }
                    }
                    }
                    #define eigen_plain_assert(x) assert(x)
                  #else
                    // work around bug 89
                    #include <cstdlib>   // for abort
                    #include <iostream>  // for std::cerr
                
                    namespace Eigen {
                    namespace internal {
                    // trivial function copying a bool. Must be EIGEN_DONT_INLINE, so we implement it after including Eigen headers.
                    // see bug 89.
                    namespace {
    10529126 ->     EIGEN_DONT_INLINE bool copy_bool(bool b) { return b; }
                    }
                    inline void assert_fail(const char *condition, const char *function, const char *file, int line)
       ##### ->     {
                      std::cerr << "assertion failed: " << condition << " in function " << function << " at " << file << ":" << line << std::endl;
                      abort();
                    }
                    }
                    }
                    #define eigen_plain_assert(x) \
                      do { \
                        if(!Eigen::internal::copy_bool(x)) \
                          Eigen::internal::assert_fail(EIGEN_MAKESTRING(x), __PRETTY_FUNCTION__, __FILE__, __LINE__); \
                      } while(false)
                  #endif
                #endif
                
                // eigen_assert can be overridden
                #ifndef eigen_assert
                #define eigen_assert(x) eigen_plain_assert(x)
                #endif
                
                #ifdef EIGEN_INTERNAL_DEBUGGING
                #define eigen_internal_assert(x) eigen_assert(x)
                #else
                #define eigen_internal_assert(x)
                #endif
                
                #ifdef EIGEN_NO_DEBUG
                #define EIGEN_ONLY_USED_FOR_DEBUG(x) (void)x
                #else
                #define EIGEN_ONLY_USED_FOR_DEBUG(x)
                #endif
                
                #ifndef EIGEN_NO_DEPRECATED_WARNING
                  #if (defined __GNUC__)
                    #define EIGEN_DEPRECATED __attribute__((deprecated))
                  #elif (defined _MSC_VER)
                    #define EIGEN_DEPRECATED __declspec(deprecated)
                  #else
                    #define EIGEN_DEPRECATED
                  #endif
                #else
                  #define EIGEN_DEPRECATED
                #endif
                
                #if (defined __GNUC__)
                #define EIGEN_UNUSED __attribute__((unused))
                #else
                #define EIGEN_UNUSED
                #endif
                
                // Suppresses 'unused variable' warnings.
                #define EIGEN_UNUSED_VARIABLE(var) (void)var;
                
                #if !defined(EIGEN_ASM_COMMENT) && (defined __GNUC__)
                #define EIGEN_ASM_COMMENT(X)  asm("#" X)
                #else
                #define EIGEN_ASM_COMMENT(X)
                #endif
                
                /* EIGEN_ALIGN_TO_BOUNDARY(n) forces data to be n-byte aligned. This is used to satisfy SIMD requirements.
                 * However, we do that EVEN if vectorization (EIGEN_VECTORIZE) is disabled,
                 * so that vectorization doesn't affect binary compatibility.
                 *
                 * If we made alignment depend on whether or not EIGEN_VECTORIZE is defined, it would be impossible to link
                 * vectorized and non-vectorized code.
                 */
                #if (defined __GNUC__) || (defined __PGI) || (defined __IBMCPP__)
                  #define EIGEN_ALIGN_TO_BOUNDARY(n) __attribute__((aligned(n)))
                #elif (defined _MSC_VER)
                  #define EIGEN_ALIGN_TO_BOUNDARY(n) __declspec(align(n))
                #elif (defined __SUNPRO_CC)
                  // FIXME not sure about this one:
                  #define EIGEN_ALIGN_TO_BOUNDARY(n) __attribute__((aligned(n)))
                #else
                  #error Please tell me what is the equivalent of __attribute__((aligned(n))) for your compiler
                #endif
                
                #define EIGEN_ALIGN16 EIGEN_ALIGN_TO_BOUNDARY(16)
                
                #if EIGEN_ALIGN_STATICALLY
                #define EIGEN_USER_ALIGN_TO_BOUNDARY(n) EIGEN_ALIGN_TO_BOUNDARY(n)
                #define EIGEN_USER_ALIGN16 EIGEN_ALIGN16
                #else
                #define EIGEN_USER_ALIGN_TO_BOUNDARY(n)
                #define EIGEN_USER_ALIGN16
                #endif
                
                #ifdef EIGEN_DONT_USE_RESTRICT_KEYWORD
                  #define EIGEN_RESTRICT
                #endif
                #ifndef EIGEN_RESTRICT
                  #define EIGEN_RESTRICT __restrict
                #endif
                
                #ifndef EIGEN_STACK_ALLOCATION_LIMIT
                #define EIGEN_STACK_ALLOCATION_LIMIT 20000
                #endif
                
                #ifndef EIGEN_DEFAULT_IO_FORMAT
                #ifdef EIGEN_MAKING_DOCS
                // format used in Eigen's documentation
                // needed to define it here as escaping characters in CMake add_definition's argument seems very problematic.
                #define EIGEN_DEFAULT_IO_FORMAT Eigen::IOFormat(3, 0, " ", "\n", "", "")
                #else
                #define EIGEN_DEFAULT_IO_FORMAT Eigen::IOFormat()
                #endif
                #endif
                
                // just an empty macro !
                #define EIGEN_EMPTY
                
                #if defined(_MSC_VER) && (!defined(__INTEL_COMPILER))
                #define EIGEN_INHERIT_ASSIGNMENT_EQUAL_OPERATOR(Derived) \
                  using Base::operator =;
                #else
                #define EIGEN_INHERIT_ASSIGNMENT_EQUAL_OPERATOR(Derived) \
                  using Base::operator =; \
                  EIGEN_STRONG_INLINE Derived& operator=(const Derived& other) \
                  { \
                    Base::operator=(other); \
                    return *this; \
                  }
                #endif
                
                #define EIGEN_INHERIT_ASSIGNMENT_OPERATORS(Derived) \
                  EIGEN_INHERIT_ASSIGNMENT_EQUAL_OPERATOR(Derived)
                
                /**
                * Just a side note. Commenting within defines works only by documenting
                * behind the object (via '!<'). Comments cannot be multi-line and thus
                * we have these extra long lines. What is confusing doxygen over here is
                * that we use '\' and basically have a bunch of typedefs with their
                * documentation in a single line.
                **/
                
                #define EIGEN_GENERIC_PUBLIC_INTERFACE(Derived) \
                  typedef typename Eigen::internal::traits<Derived>::Scalar Scalar; /*!< \brief Numeric type, e.g. float, double, int or std::complex<float>. */ \
                  typedef typename Eigen::NumTraits<Scalar>::Real RealScalar; /*!< \brief The underlying numeric type for composed scalar types. \details In cases where Scalar is e.g. std::complex<T>, T were corresponding to RealScalar. */ \
                  typedef typename Base::CoeffReturnType CoeffReturnType; /*!< \brief The return type for coefficient access. \details Depending on whether the object allows direct coefficient access (e.g. for a MatrixXd), this type is either 'const Scalar&' or simply 'Scalar' for objects that do not allow direct coefficient access. */ \
                  typedef typename Eigen::internal::nested<Derived>::type Nested; \
                  typedef typename Eigen::internal::traits<Derived>::StorageKind StorageKind; \
                  typedef typename Eigen::internal::traits<Derived>::Index Index; \
                  enum { RowsAtCompileTime = Eigen::internal::traits<Derived>::RowsAtCompileTime, \
                        ColsAtCompileTime = Eigen::internal::traits<Derived>::ColsAtCompileTime, \
                        Flags = Eigen::internal::traits<Derived>::Flags, \
                        CoeffReadCost = Eigen::internal::traits<Derived>::CoeffReadCost, \
                        SizeAtCompileTime = Base::SizeAtCompileTime, \
                        MaxSizeAtCompileTime = Base::MaxSizeAtCompileTime, \
                        IsVectorAtCompileTime = Base::IsVectorAtCompileTime };
                
                
                #define EIGEN_DENSE_PUBLIC_INTERFACE(Derived) \
                  typedef typename Eigen::internal::traits<Derived>::Scalar Scalar; /*!< \brief Numeric type, e.g. float, double, int or std::complex<float>. */ \
                  typedef typename Eigen::NumTraits<Scalar>::Real RealScalar; /*!< \brief The underlying numeric type for composed scalar types. \details In cases where Scalar is e.g. std::complex<T>, T were corresponding to RealScalar. */ \
                  typedef typename Base::PacketScalar PacketScalar; \
                  typedef typename Base::CoeffReturnType CoeffReturnType; /*!< \brief The return type for coefficient access. \details Depending on whether the object allows direct coefficient access (e.g. for a MatrixXd), this type is either 'const Scalar&' or simply 'Scalar' for objects that do not allow direct coefficient access. */ \
                  typedef typename Eigen::internal::nested<Derived>::type Nested; \
                  typedef typename Eigen::internal::traits<Derived>::StorageKind StorageKind; \
                  typedef typename Eigen::internal::traits<Derived>::Index Index; \
                  enum { RowsAtCompileTime = Eigen::internal::traits<Derived>::RowsAtCompileTime, \
                        ColsAtCompileTime = Eigen::internal::traits<Derived>::ColsAtCompileTime, \
                        MaxRowsAtCompileTime = Eigen::internal::traits<Derived>::MaxRowsAtCompileTime, \
                        MaxColsAtCompileTime = Eigen::internal::traits<Derived>::MaxColsAtCompileTime, \
                        Flags = Eigen::internal::traits<Derived>::Flags, \
                        CoeffReadCost = Eigen::internal::traits<Derived>::CoeffReadCost, \
                        SizeAtCompileTime = Base::SizeAtCompileTime, \
                        MaxSizeAtCompileTime = Base::MaxSizeAtCompileTime, \
                        IsVectorAtCompileTime = Base::IsVectorAtCompileTime }; \
                  using Base::derived; \
                  using Base::const_cast_derived;
                
                
                #define EIGEN_PLAIN_ENUM_MIN(a,b) (((int)a <= (int)b) ? (int)a : (int)b)
                #define EIGEN_PLAIN_ENUM_MAX(a,b) (((int)a >= (int)b) ? (int)a : (int)b)
                
                // EIGEN_SIZE_MIN_PREFER_DYNAMIC gives the min between compile-time sizes. 0 has absolute priority, followed by 1,
                // followed by Dynamic, followed by other finite values. The reason for giving Dynamic the priority over
                // finite values is that min(3, Dynamic) should be Dynamic, since that could be anything between 0 and 3.
                #define EIGEN_SIZE_MIN_PREFER_DYNAMIC(a,b) (((int)a == 0 || (int)b == 0) ? 0 \
                                           : ((int)a == 1 || (int)b == 1) ? 1 \
                                           : ((int)a == Dynamic || (int)b == Dynamic) ? Dynamic \
                                           : ((int)a <= (int)b) ? (int)a : (int)b)
                
                // EIGEN_SIZE_MIN_PREFER_FIXED is a variant of EIGEN_SIZE_MIN_PREFER_DYNAMIC comparing MaxSizes. The difference is that finite values
                // now have priority over Dynamic, so that min(3, Dynamic) gives 3. Indeed, whatever the actual value is
                // (between 0 and 3), it is not more than 3.
                #define EIGEN_SIZE_MIN_PREFER_FIXED(a,b)  (((int)a == 0 || (int)b == 0) ? 0 \
                                           : ((int)a == 1 || (int)b == 1) ? 1 \
                                           : ((int)a == Dynamic && (int)b == Dynamic) ? Dynamic \
                                           : ((int)a == Dynamic) ? (int)b \
                                           : ((int)b == Dynamic) ? (int)a \
                                           : ((int)a <= (int)b) ? (int)a : (int)b)
                
                // see EIGEN_SIZE_MIN_PREFER_DYNAMIC. No need for a separate variant for MaxSizes here.
                #define EIGEN_SIZE_MAX(a,b) (((int)a == Dynamic || (int)b == Dynamic) ? Dynamic \
                                           : ((int)a >= (int)b) ? (int)a : (int)b)
                
                #define EIGEN_LOGICAL_XOR(a,b) (((a) || (b)) && !((a) && (b)))
                
                #define EIGEN_IMPLIES(a,b) (!(a) || (b))
                
                #define EIGEN_MAKE_CWISE_BINARY_OP(METHOD,FUNCTOR) \
                  template<typename OtherDerived> \
                  EIGEN_STRONG_INLINE const CwiseBinaryOp<FUNCTOR<Scalar>, const Derived, const OtherDerived> \
                  (METHOD)(const EIGEN_CURRENT_STORAGE_BASE_CLASS<OtherDerived> &other) const \
                  { \
                    return CwiseBinaryOp<FUNCTOR<Scalar>, const Derived, const OtherDerived>(derived(), other.derived()); \
                  }
                
                // the expression type of a cwise product
                #define EIGEN_CWISE_PRODUCT_RETURN_TYPE(LHS,RHS) \
                    CwiseBinaryOp< \
                      internal::scalar_product_op< \
                          typename internal::traits<LHS>::Scalar, \
                          typename internal::traits<RHS>::Scalar \
                      >, \
                      const LHS, \
                      const RHS \
                    >
                
                #endif // EIGEN_MACROS_H


Top 10 Lines:

     Line      Count

      203   10529126

Execution Summary:

        2   Executable lines in this file
        2   Lines executed
   100.00   Percent of the file executed

 10529126   Total number of line executions
5264563.00   Average executions per line
